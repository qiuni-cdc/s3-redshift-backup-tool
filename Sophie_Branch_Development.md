# Q&A - Common Issues and Solutions

## ModuleNotFoundError: cdc_backup_integration Issue

### Why it happens:
- The file `src/backup/cdc_backup_integration.py` is missing from the codebase
- `src/backup/row_based.py` line 16 imports `from src.backup.cdc_backup_integration import create_cdc_integration`
- Python fails at import time when it can't find the module

### How it connects to `python -m src.cli.main status`:
**Import Chain:**
1. `status` command â†’ uses `ConnectionManager` for health checks
2. `ConnectionManager` â†’ imports backup strategies 
3. `RowBasedBackupStrategy` â†’ imports `cdc_backup_integration`
4. **Import fails** â†’ entire module loading fails

### Key insight:
Even though the `status` command doesn't directly use CDC functionality, Python resolves **all imports** when loading modules. The missing import causes the entire chain to fail, preventing the status command from running.

### Solution:
Create the missing `src/backup/cdc_backup_integration.py` file with a `create_cdc_integration()` function to satisfy the import requirement.  


## S3 Pagination Issue - 371/400 Files Discovery Problem

### Why it happens:
- S3 `list_objects_v2()` has maximum 1000 objects per call limit
- Single call only returns first 1000 objects under prefix
- Missing 29 parquet files were in objects beyond the 1000 limit
- `check_processed_files.py` used pagination and found all 400 files correctly

### Root cause in `src/core/gemini_redshift_loader.py`:
```python
# BROKEN: Single call, limited results
response = s3_client.list_objects_v2(
    Bucket=self.config.s3.bucket_name,
    Prefix=prefix,
    MaxKeys=max_keys  # Even 2000 limit doesn't override 1000 default
)
```

### Fix Applied:
```python
# FIXED: Pagination gets ALL objects
paginator = s3_client.get_paginator('list_objects_v2')
page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)
for page in page_iterator:
    all_objects.extend(page['Contents'])
```

### Key insight:
Even though S3 appears to scan "1000 total objects", the missing files were simply not discovered in the initial scan. Pagination ensures complete file discovery across all S3 pages.

### Result:
- Before fix: 371/400 files discovered
- After fix: All 400/400 files discovered  
- Enhanced logging shows file-by-file processing with detailed failure analysis 

## Batch size for massive table
 Here are my recommendations for tuning a massive table sync:

  1. Database batch_size (in your .yml pipeline file)

  This setting controls how many rows are fetched from MySQL in a single query. Your current setting is 100000.

   * Recommendation: Slightly increase this value. For very large tables, the overhead of many small queries can add up.
     A larger batch size reduces the number of round-trips to the database.
   * Trade-offs:
       * Pros: Fewer, more efficient queries against the source database. Can increase overall throughput.
       * Cons: Increases memory usage on the machine running the script, as it has to hold all these rows in memory. A
         very large value could also put more strain on the source database.
   * Suggested Value: Try increasing this to 250000 or 500000. Monitor the memory usage of the Python process. If it gets
     too high, bring this number back down.

  2. BACKUP_BATCH_SIZE (in your .env file)

  This setting controls how many rows are written into each individual Parquet file that gets uploaded to S3. Your
  current default is 50000.

   * Recommendation: Adjust this to create files between 10MB and 200MB. This is the optimal size range for Redshift's
     COPY command to achieve maximum parallelism.
   * Analysis: You've observed that 50,000 rows create a ~7MB file. This is good, but on the smaller side. Creating
     slightly larger files will reduce the total number of files S3 and Redshift have to manage, which can improve
     performance.
   * Suggested Value: Increasing this to 100000 (to create ~14MB files) or 250000 (to create ~35MB files) would be an
     excellent tuning step. It's generally not recommended to go above 500000 rows per file unless your rows are very
     small.

## Watermark Tracking Bug Issues (Resolved)

### Issue 1: Processed Files During Backup Phase

**Problem**: Files created during backup (MySQL â†’ S3) were incorrectly marked as "processed" (loaded to Redshift).

**Symptoms**: 
- Files show in "Processed S3 Files" but haven't been loaded to Redshift
- `--redshift-only` skips files that should be loaded
- Data appears processed but isn't actually in Redshift

**Root Cause**: Backup stage updated `processed_s3_files` instead of tracking backup files separately.

**Fix Applied**:
```python
# BEFORE: Incorrectly marked as processed during backup
watermark_data = {
    'processed_s3_files': backup_files  # âŒ Wrong - these aren't loaded yet
}

# AFTER: Separate tracking for backup vs loaded files
watermark_data = {
    'backup_s3_files': backup_files,     # âœ… Files awaiting Redshift load
    's3_file_count': len(backup_files),  # âœ… Count of backup files
    # processed_s3_files only updated by Redshift loader
}
```

**Result**: 
- `ğŸ“¦ Backup S3 Files (awaiting Redshift load)` - Shows files created during backup
- `âœ… Processed S3 Files (loaded to Redshift)` - Shows files actually loaded
- `--redshift-only` correctly finds unprocessed files

### Issue 2: Watermark Loss on Interruption

**Problem**: When sync was interrupted, watermark showed "0 rows extracted, 0 S3 files" despite files existing.

**Symptoms**:
- Created 3 parquet files but watermark shows 0 files
- Rows extracted counter resets to 0 after interruption
- Progress lost on restart

**Root Cause**: Watermark only updated at final completion, not incrementally during chunks.

**Fix Applied**:
```python
# BEFORE: Only updated at final completion
def final_watermark_update():
    # If interrupted, all progress lost

# AFTER: Incremental updates after each chunk
def _update_chunk_watermark_with_files():
    watermark_data = {
        'mysql_rows_extracted': total_rows_processed,  # âœ… Track progress
        's3_file_count': len(all_backup_files),       # âœ… Track file count
        'backup_s3_files': all_backup_files           # âœ… Track file list
    }
```

**Result**: 
- Watermark shows accurate progress after each chunk
- Safe resume from exact interruption point
- No data loss when sync is interrupted

### Issue 3: Row Count Double-Counting

**Problem**: Watermark showed incorrect row counts (60,000 instead of actual 50,000).

**Symptoms**:
- Watermark shows more rows than actually exist in files
- Session-based counting adds incorrectly across runs
- File count correct but row count inflated

**Root Cause**: Session totals being added instead of using actual file content.

**Fix Applied**:
```python
# BEFORE: Used session processing totals (additive)
rows_extracted = session_rows_processed  # Could double-count

# AFTER: Count actual rows from parquet files
actual_rows_in_files = self._count_rows_in_backup_s3_files(table_name)
rows_extracted = actual_rows_in_files    # Always accurate
mode = 'absolute'                        # Prevent additive errors
```

**Result**: 
- Watermark always shows actual row count from files
- No more double-counting across sessions
- Consistent and accurate progress tracking

### Issue 4: Field Preservation During Updates

**Problem**: New `backup_s3_files` field lost during watermark updates.

**Symptoms**:
- `backup_s3_files` shows correctly but `s3_file_count` shows 0
- Field inconsistencies between display and storage

**Root Cause**: Watermark update methods didn't preserve new fields.

**Fix Applied**:
```python
# Enhanced update_mysql_watermark to sync fields
if hasattr(watermark, 'backup_s3_files') and watermark.backup_s3_files:
    watermark.s3_file_count = len(watermark.backup_s3_files)  # âœ… Sync count
    
# _update_watermark_direct uses setattr to handle any field
for field, value in watermark_data.items():
    if hasattr(watermark, field):
        setattr(watermark, field, value)  # âœ… Handles all fields
```

**Result**: 
- All watermark fields stay consistent
- New fields properly preserved across updates
- Display matches actual stored data

### Recovery Commands

**Fix current watermark issues**:
```bash
# Count actual rows in files and fix watermark
python fix_current_watermark.py

# Check watermark details
python -m src.cli.main watermark get -t table_name --show-files

# Fix any counter discrepancies  
python fix_watermark_counters.py table_name
```

### Prevention Measures Applied

- âœ… **Incremental progress tracking** prevents data loss on interruption
- âœ… **Separate file tracking** prevents incorrect processing status  
- âœ… **Actual file counting** ensures accurate row counts
- âœ… **Field preservation** maintains watermark consistency
- âœ… **Diagnostic utilities** for troubleshooting and recovery

### Issue 5: File Movement and S3 File Count Display (Resolved)

**Problem**: After processing files to Redshift, files appeared in both backup and processed lists, and "S3 Files Created" showed 0.

**Symptoms**:
- Duplicate files in both `backup_s3_files` and `processed_s3_files`
- "S3 Files Created: 0" despite files being successfully processed
- Confusion about which files are awaiting Redshift load vs already loaded

**Root Cause**: 
1. Missing logic to move files from backup list to processed list when loaded to Redshift
2. Incorrect calculation of `s3_file_count` after moving files

**Fix Applied**:
```python
# FIXED: Move files from backup_s3_files to processed_s3_files
if hasattr(watermark, 'backup_s3_files') and watermark.backup_s3_files:
    original_backup_count = len(watermark.backup_s3_files)
    watermark.backup_s3_files = [
        backup_file for backup_file in watermark.backup_s3_files
        if backup_file not in processed_files
    ]
    moved_files_count = original_backup_count - len(watermark.backup_s3_files)
    
    if moved_files_count > 0:
        logger.info(f"Moved {moved_files_count} files from backup_s3_files to processed_s3_files")
        
    # Update s3_file_count to reflect total files (backup + processed)
    backup_count = len(watermark.backup_s3_files) if watermark.backup_s3_files else 0
    processed_count = len(watermark.processed_s3_files) if watermark.processed_s3_files else 0
    watermark.s3_file_count = backup_count + processed_count
```

**Result**: 
- Files properly move from backup to processed when loaded to Redshift
- No more duplicate files in both lists
- `s3_file_count` correctly shows total files created (backup + processed)
- Clear distinction between files awaiting load vs files already loaded
- "S3 Files Created" now displays the correct total count

## DateTime Parsing Error in Backup Logic (Resolved)

**Problem**: Backup operations failed with `fromisoformat: argument must be str` TypeError during timestamp processing.

**Symptoms**:
- Error: `'fromisoformat: argument must be str'`
- Backup operations failed with TypeError
- Error occurred in row_based_backup strategy with timestamp_id chunking

**Error Example**:
```
2025-09-05T19:11:42.861457Z [error] Operation failed [backup] 
chunking_type=timestamp_id error_message='fromisoformat: argument must be str' 
error_type=TypeError table_name=US_DW_UNIDW_SSH:unidw.dw_parcel_pricing_temp
```

**Root Cause**: 
Mixed datetime/string type handling in watermark timestamp processing. The code assumed `last_mysql_data_timestamp` was always a string, but sometimes it was already a datetime object.

**Fix Applied**:
```python
# BEFORE (BUGGY):
data_timestamp_dt = datetime.fromisoformat(watermark.last_mysql_data_timestamp.replace('Z', '+00:00'))

# AFTER (FIXED):
if isinstance(last_timestamp, str):
    data_timestamp_dt = datetime.fromisoformat(last_timestamp.replace('Z', '+00:00'))
else:
    # Handle case where last_timestamp is already a datetime object
    data_timestamp_dt = last_timestamp
```

**Result**: 
- Backup operations handle both string and datetime timestamp inputs gracefully
- No more TypeError failures during timestamp processing
- Defensive programming prevents type assumption errors
- Consistent datetime handling throughout watermark system

## Custom Redshift Optimization Feature

### Overview
The system supports customized DISTKEY and SORTKEY optimization for Redshift tables through a configuration-driven approach, allowing fine-tuned performance optimization for different table patterns.

### Configuration File: `redshift_keys.json`
Tables can be optimized by creating a `redshift_keys.json` file in the project root:

```json
{
  "settlement.settlement_normal_delivery_detail": {
    "distkey": "ant_parcel_no",
    "sortkey": {
      "type": "compound",
      "columns": ["billing_num", "create_at"]
    }
  },
  "settlement.settle_orders": {
    "distkey": "tracking_number",
    "sortkey": {
      "type": "interleaved", 
      "columns": ["tracking_number", "created_at"]
    }
  }
}
```

### DISTKEY Configuration
- **Purpose**: Controls data distribution across Redshift nodes for optimal join performance
- **Usage**: `"distkey": "column_name"`
- **Validation**: System verifies column exists in table schema
- **Fallback**: Uses `DISTSTYLE AUTO` if column not found or not specified

### SORTKEY Configuration Formats

#### 1. Dictionary Format (Recommended)
```json
"sortkey": {
  "type": "compound|interleaved",
  "columns": ["col1", "col2", "col3"]
}
```

#### 2. Array Format (Legacy)
```json
"sortkey": ["col1", "col2"]  // Defaults to COMPOUND
```

#### 3. String Format (Single Column)
```json
"sortkey": "single_column"  // Defaults to COMPOUND
```

### SORTKEY Types
- **COMPOUND**: Best for queries filtering on prefix columns (recommended for most cases)
- **INTERLEAVED**: Best for queries filtering on any combination of sort columns

### Generated DDL Examples

**No Configuration (Default)**:
```sql
CREATE TABLE IF NOT EXISTS schema.table_name (
  columns...
)
DISTSTYLE AUTO
SORTKEY AUTO;
```

**With Custom Configuration**:
```sql
CREATE TABLE IF NOT EXISTS schema.settlement_normal_delivery_detail (
  columns...
)
DISTKEY(ant_parcel_no)
COMPOUND SORTKEY(billing_num, create_at);
```

### Implementation Features
- **Schema Validation**: Verifies all specified columns exist before applying
- **Column Name Sanitization**: Handles column names starting with numbers (e.g., `190_time` â†’ `col_190_time`)
- **Graceful Fallback**: Uses AUTO optimization if custom columns not found
- **Comprehensive Logging**: Reports which optimizations are applied or skipped

### Use Cases
1. **High-Volume Tables**: Use DISTKEY on commonly joined columns
2. **Time-Series Data**: Use time-based SORTKEY for range queries
3. **Join Optimization**: Match DISTKEY with frequent join columns
4. **Query Patterns**: Choose COMPOUND vs INTERLEAVED based on WHERE clauses

### Testing
The feature includes comprehensive testing in `tests/test_core/test_redshift_optimization.py` that verifies:
- Custom configurations are applied correctly
- Default AUTO optimization works when no config provided
- Column validation and sanitization work properly
- Different SORTKEY types generate correct DDL 

## Batch Size Configuration Architecture Fix

### Problem: Multiple Conflicting Batch Size Systems

The system had **three different** batch size concepts causing confusion and inconsistent file sizes:

1. **Chunk Size** (rows fetched from MySQL): From pipeline/table config
2. **S3 Batch Size** (rows per Parquet file): From environment variable  
3. **System Defaults** (hardcoded fallbacks): Scattered throughout code

### Root Cause Analysis

**Configuration Hierarchy Issues:**
- Pipeline config: `batch_size: 500000` (chunk size)
- Environment variable: `BACKUP_BATCH_SIZE=50000` (S3 batch size)
- Hardcoded fallbacks: `50000` in CDC config, `10000` in settings.py
- **Result**: 1 chunk = 10 Parquet files (500K Ã· 50K = 10)

**Code Locations with Hardcoded Values:**
```python
# PROBLEM: Multiple hardcoded 50000 values
src/core/cdc_configuration_manager.py:59: batch_size=...get('cdc_batch_size', 50000)
src/core/cdc_configuration_manager.py:232: 'batch_size': 50000
src/core/cdc_strategy_engine.py:116: batch_size: int = 50000
src/config/settings.py:134: batch_size: int = Field(10000, ...)
```

**Environment Variable Override:**
```bash
# .env file had conflicting values
BACKUP_BATCH_SIZE=50000  # This controlled S3 file size
# But validation showed "50000 rows" per file instead of expected chunk size
```

### Fix Applied: Unified Configuration Hierarchy

#### 1. **Fixed Batch Size Priority Chain**
```python
# BEFORE: Inconsistent fallbacks
batch_size = table_config.get('batch_size', 50000)  # Hardcoded

# AFTER: Proper hierarchy
# 1. Table-specific processing.batch_size (highest priority)
batch_size = table_config.processing.get('batch_size')
if batch_size is None:
    # 2. Pipeline processing.batch_size (fallback)
    pipeline_processing = config.pipeline.processing if hasattr(config.pipeline, 'processing') else {}
    batch_size = pipeline_processing.get('batch_size')
if batch_size is None:
    # 3. System default from AppConfig (final fallback)
    batch_size = config.backup.target_rows_per_chunk
```

#### 2. **Environment Configuration Fix**
```bash
# BEFORE (.env):
BACKUP_BATCH_SIZE=50000  # Created 10 small files per chunk

# AFTER (.env):
BACKUP_BATCH_SIZE=500000  # Creates 1 large file per chunk
```

#### 3. **System Default Updates**
```python
# Updated settings.py system defaults
target_rows_per_chunk: int = Field(100000, ...)  # Was 500000
```

#### 4. **CDC Configuration Manager Fix**
```python
# BEFORE: Hardcoded fallback
batch_size=table_config.get('batch_size', table_config.get('cdc_batch_size', 50000))

# AFTER: Dynamic fallback
batch_size=table_config.get('batch_size', table_config.get('cdc_batch_size',
    table_config.get('system_default_batch_size', 100000)))
```

### Current Configuration Structure

**Your Pipeline Config:**
```yaml
pipeline:
  processing:
    batch_size: 100000  # Pipeline default

tables:
  unidw.dw_parcel_detail_tool:
    processing:
      batch_size: 1000000  # Table-specific override (highest priority)
```

**Environment Settings:**
```bash
BACKUP_BATCH_SIZE=500000  # S3 file size (rows per Parquet file)
```

### Result: Configurable File Granularity

**Before Fix:**
- Chunk: 500,000 rows â†’ 10 files Ã— 50,000 rows each
- Validation: "50000 rows" per file
- Many small files to manage

**After Fix:**
- Chunk: 1,000,000 rows â†’ 2 files Ã— 500,000 rows each
- Validation: "500000 rows" per file
- Optimal file sizes for Redshift COPY performance

### Configuration Best Practices Applied

1. **Table-Specific Override**: Use for large tables requiring bigger batches
2. **Pipeline Default**: Set reasonable baseline for all tables in pipeline
3. **System Fallback**: Provides safe defaults when no config specified
4. **Environment Control**: Fine-tune S3 file sizes without code changes

### Key Benefits

- âœ… **Consistent Configuration**: Single source of truth per level
- âœ… **Performance Optimization**: Right-sized files for Redshift COPY
- âœ… **Flexible Tuning**: Override at table/pipeline/system level as needed
- âœ… **No More Hardcoding**: All defaults configurable through proper channels
- âœ… **Fewer S3 Files**: Reduced management overhead with optimally sized files


## Table Name Pattern Matching Fix

### Issue: S3 File Discovery Failures

**Problem Identified:**
- Table name: `US_DW_UNIDW_SSH:unidw.dw_parcel_detail_tool`  
- Actual S3 files: `us_dw_unidw_ssh_unidw_dw_parcel_detail_tool_*.parquet`
- Pattern matching returned: **0 files found** (should be 46 files)

**Root Cause:** Multiple components had inconsistent table name normalization logic:
- Some normalized: `US_DW_UNIDW_SSH:unidw_dw_parcel_detail_tool` (only `.` â†’ `_`)
- Actual S3 files: `us_dw_unidw_ssh_unidw_dw_parcel_detail_tool` (lowercase + `:` â†’ `_`)

### Components Fixed

**1. `src/backup/row_based.py:650` - S3 File Counting**
```python
# BEFORE (BROKEN):
safe_table_name = table_name.replace('.', '_')
# Result: "US_DW_UNIDW_SSH:unidw_dw_parcel_detail_tool"

# AFTER (FIXED):
safe_table_name = table_name.lower().replace(':', '_').replace('.', '_')
# Result: "us_dw_unidw_ssh_unidw_dw_parcel_detail_tool"
```

**2. `src/core/gemini_redshift_loader.py:596-607` - Redshift S3 File Discovery**
```python
# BEFORE (BROKEN):
if ':' in table_name:
    # Clean table: replace dots with underscores (NO lowercase)
    clean_table = actual_table.replace('.', '_').replace('-', '_')

# AFTER (FIXED):
if ':' in table_name:
    # Clean table: lowercase and replace dots with underscores
    clean_table = actual_table.lower().replace('.', '_').replace('-', '_')
```

### Symptoms Fixed

**Before Fix:**
```
Actual S3 file count for US_DW_UNIDW_SSH:unidw.dw_parcel_detail_tool: 0 files
No successful MySQL backup found for US_DW_UNIDW_SSH:unidw.dw_parcel_detail_tool
No S3 parquet files found for US_DW_UNIDW_SSH:unidw.dw_parcel_detail_tool
```

**After Fix:**
```
Actual S3 file count for US_DW_UNIDW_SSH:unidw.dw_parquet_detail_tool: 46 files
Found S3 files for processing: 46 parquet files
Successfully loaded table data to Redshift
```

### Session Stuck Problem Resolved

**Why Sessions Got Stuck (0 file count):**
1. **Watermark Inconsistency**: System believed no files exist (count=0) while 46 files actually existed in S3
2. **State Confusion**: Backup logic couldn't reconcile actual S3 state vs tracked watermark state  
3. **Loop Logic Issues**: Main processing loop continued indefinitely due to inconsistent progress tracking
4. **Resource Waste**: Sessions running endlessly thinking they needed to process already-existing data

**Resolution:** Accurate file counting enables proper watermark tracking and session completion decisions.


## Watermark MySQL Status Fix

### Issue: Redshift Loader Rejecting Valid S3 Files

**Problem:** Loader required `mysql_status: success` but watermark showed `mysql_status: in_progress`

**Watermark State:**
```
MySQL â†’ S3 Backup Stage: Status: in_progress
S3 â†’ Redshift Loading Stage: Status: success  
S3 Files Created: 46
Backup S3 Files (awaiting Redshift load): 46 files exist
```

### Root Cause in `src/core/gemini_redshift_loader.py:310-312`

**Before Fix (RESTRICTIVE):**
```python
if not watermark or watermark.mysql_status != 'success':
    logger.warning(f"No successful MySQL backup found for {table_name}")
    return []  # BLOCKS loading of existing S3 files
```

**After Fix (PRAGMATIC):**
```python
if not watermark:
    logger.warning(f"No watermark found for {table_name}")
    return []

# Allow loading with in_progress or success status (files may exist from interrupted backup)
if watermark.mysql_status not in ['success', 'in_progress']:
    logger.warning(f"MySQL backup status is {watermark.mysql_status} for {table_name}, skipping")
    return []

if watermark.mysql_status == 'in_progress':
    logger.info(f"MySQL backup is in_progress for {table_name}, checking for existing S3 files")
```

### Business Logic Justification

**Scenario:** Backup was interrupted but S3 files were successfully created
- MySQL backup status: `in_progress` (process was interrupted)  
- S3 files: 46 files exist and are valid
- Business need: Load these files to Redshift (they contain real data)

**Old Logic:** Reject all loading because status isn't `success`
**New Logic:** Allow loading if files exist, even with `in_progress` status

### Impact

**Before:** 46 valid S3 files couldn't be loaded due to status check
**After:** 46 S3 files successfully processed to Redshift regardless of interrupted backup status

**Key Insight:** Watermark status should guide processing decisions, not block valid data loading when files demonstrably exist.

## Multi-Connection Password Management Integration (Resolved)

### Issue: Database Password Configuration for Multi-Pipeline Support

**Problem:** System used single `DB_PASSWORD` environment variable but pipeline configurations require connection-specific passwords (US DW, Canada DW, Production, QA, etc.).

**Symptoms:**
```
2025-09-16T20:39:59.518831Z [warning] No password found in DatabaseConfig or environment variables
```

**Root Cause:** 
1. **ConnectionRegistry Available**: `connections.yml` defined 14+ connection configurations with environment variable references
2. **ConnectionManager Not Integrated**: System fell back to basic `DatabaseConfig` instead of using ConnectionRegistry
3. **Missing Password**: No generic `DB_PASSWORD` in `.env`, only connection-specific passwords like:
   - `DB_US_DW_RO_PASSWORD` (for US Data Warehouse connections)
   - `DB_CA_DW_RO_PASSWORD` (for Canada connections)  
   - `DB_US_QA_PASSWORD` (for QA environment)
   - `DB2_US_RO_PASSWORD` (for DB2 connections)

### Connection Architecture Overview

**Your `.env` Connection-Specific Passwords:**
```bash
DB_US_DW_RO_PASSWORD=qPwRT]9(BUI0Q2Vw     # US Data Warehouse
DB_CA_DW_RO_PASSWORD=cJY(,,%+sbam{%9`     # Canada Data Warehouse  
DB_US_PROD_RO_PASSWORD=L~yL&Yw9t?qZn=k~   # US Production
DB_US_QA_PASSWORD=3B/l0PB9}fXRlebm        # US QA Environment
DB2_US_RO_PASSWORD=qPwRT]9(BUI0Q2Vw       # DB2 Instance
```

**Your `connections.yml` Configuration:**
```yaml
connections:
  sources:
    mysql_default:
      password: "${DB_US_DW_PASSWORD}"  # References environment variable
    US_DW_UNIDW_SSH:
      password: "${DB_US_DW_RO_PASSWORD}"  # Connection-specific password
    CA_DW_RO_SSH:
      password: "${DB_CA_DW_RO_PASSWORD}"  # Canada-specific password
```

### Fix Applied: ConnectionRegistry Integration

**1. Enhanced ConnectionManager (`src/core/connections.py:67-76`)**
```python
# BEFORE: Only checked basic DatabaseConfig
if self.connection_registry and connection_name:
    try:
        conn_config = self.connection_registry.get_connection(connection_name)
        if conn_config:
            logger.info(f"Using connection configuration for: {connection_name}")
            return conn_config
    except Exception as e:
        logger.warning(f"Failed to get connection {connection_name} from registry: {e}")
        
# Fallback still happened due to missing connection names in health_check()
```

**2. Updated Health Check Method (`src/core/connections.py:612,620`)**
```python
# BEFORE (PROBLEMATIC): No connection name passed
with self.ssh_tunnel() as local_port:
    with self.database_connection(local_port) as conn:

# AFTER (FIXED): Uses mysql_default connection
with self.ssh_tunnel('mysql_default') as local_port:
    with self.database_connection(local_port, 'mysql_default') as conn:
```

**3. Optional Password Field (`src/config/settings.py:13`)**
```python
# BEFORE: Required password field
password: SecretStr = Field(..., description="Database password")

# AFTER: Optional password field (allows connection-specific passwords)
password: Optional[SecretStr] = Field(None, description="Database password")
```

### Test Script Enhancement

**Environment Variable Loading Fix (`test_connection_resolution.py:13-36`)**
```python
# Added automatic .env loading with fallback
try:
    from dotenv import load_dotenv
    env_file = Path(__file__).parent / '.env'
    if env_file.exists():
        load_dotenv(env_file)
        print(f"âœ… Loaded environment variables from {env_file}")
except ImportError:
    # Manual .env loading as fallback
    with open(env_file) as f:
        for line in f:
            # Parse KEY=VALUE pairs
```

### Resolution Workflow

**Test Results Confirm Integration Success:**
```
=== Environment Variables Test ===
âœ… DB_US_DW_PASSWORD: **************** (length: 16)
âœ… DB_US_DW_RO_PASSWORD: **************** (length: 16)
âœ… DB_CA_DW_RO_PASSWORD: **************** (length: 16)
[... all connection-specific passwords loaded]

=== ConnectionRegistry Test ===
âœ… ConnectionRegistry initialized with 14 connections
âœ… Found connection: US_DW_UNIDW_SSH
   Host: us-west-2.ro.db.analysis.uniuni.com.internal
   Password: ****************

=== ConnectionManager Test ===
âœ… ConnectionManager loaded
   Connection: US_DW_UNIDW_SSH
   Password: ****************
```

### Impact & Benefits

**Before Fix:**
- System tried to use single `DB_PASSWORD` for all connections
- Health checks failed with "No password found" warnings
- No support for multi-pipeline environments with different credentials

**After Fix:**
- Each connection uses its specific environment variable (e.g., `${DB_US_DW_RO_PASSWORD}`)
- ConnectionRegistry provides proper credential resolution
- Health checks use `mysql_default` connection for system status
- Full support for multi-environment, multi-pipeline configurations

**Key Insight:** The system now properly leverages your existing multi-connection architecture instead of requiring a generic password. Each pipeline can use its appropriate credentials while maintaining security through environment variable references.  

## Watermark Logic - Final Architecture (Resolved)

### Overview

The watermark system implements a **dual-tracking architecture** that separates cumulative totals from session-specific metrics, providing both historical progress tracking and per-execution visibility.

### Core Design Principles

#### 1. **Separation of Concerns**
- **Cumulative Metrics**: Track lifetime totals across all sync sessions
- **Session Metrics**: Track current execution only (essential for Airflow/monitoring)

#### 2. **Two-Phase Update Pattern**

**Phase 1: Batch Processing (Incremental Updates)**
Location: `row_based.py:508-516`

```python
# During batch processing - accumulate rows incrementally
self.watermark_manager.simple_manager.update_mysql_state(
    table_name=table_name,
    timestamp=accumulated_last_timestamp,
    id=accumulated_last_id,
    status='in_progress',
    rows_extracted=batch_rows,              # Adds to cumulative total
    s3_files_created=batch_files,           # Adds to cumulative total
    session_rows_total=total_rows_processed # Show session progress so far
)
```

**Behavior**:
- `rows_extracted=100` â†’ Adds 100 to cumulative `total_rows`
- `session_rows_total=total_rows_processed` â†’ Shows cumulative session progress (e.g., after 2 batches: 200)
- Updates `last_session_rows = total_rows_processed` (accumulates, not overwrites!)
- Provides resume capability if sync is interrupted
- **CRITICAL**: Even if interrupted, watermark shows correct session total

**Phase 2: Final Update (Session Total)**
Location: `row_based.py:1796-1803`

```python
# After all batches complete - set final session total
success = self.watermark_manager.simple_manager.update_mysql_state(
    table_name=table_name,
    timestamp=max_data_timestamp.isoformat(),
    id=last_processed_id,
    status=status,
    error=error_message,
    rows_extracted=0,                      # Don't add again (already accumulated)
    session_rows_total=session_rows_processed  # Set session display total
)
```

**Behavior**:
- `rows_extracted=0` â†’ No change to cumulative total (avoid double-counting)
- `session_rows_total=300` â†’ Sets `last_session_rows = 300` directly
- Changes status from `in_progress` â†’ `success`

#### 3. **Watermark Manager Implementation**
Location: `simple_watermark_manager.py:98-137`

```python
def update_mysql_state(self, table_name: str,
                      rows_extracted: Optional[int] = None,
                      session_rows_total: Optional[int] = None) -> None:
    """
    Dual-mode update supporting both cumulative and session tracking
    """
    watermark = self.get_watermark(table_name)

    # CUMULATIVE: Add new rows to existing total
    session_rows = 0
    if rows_extracted is not None and rows_extracted > 0:
        current_rows = watermark['mysql_state'].get('total_rows', 0)
        total_rows = current_rows + rows_extracted
        session_rows = rows_extracted
        logger.info(f"Cumulative update: {current_rows} + {rows_extracted} = {total_rows}")
    else:
        total_rows = watermark['mysql_state'].get('total_rows', 0)

    # SESSION: Allow direct setting of session total (for final updates)
    if session_rows_total is not None:
        session_rows = session_rows_total
        logger.info(f"Session total set directly: {session_rows_total}")

    watermark['mysql_state'].update({
        'total_rows': total_rows,           # Cumulative across all sessions
        'last_session_rows': session_rows,  # Current session only
        'last_updated': datetime.now(timezone.utc).isoformat()
    })

    self._save_watermark(table_name, watermark)
```

### Data Flow Example

**Scenario: Sync processes 300 rows across 3 batches**

```
Initial State:
  total_rows: 400
  last_session_rows: 0

Batch 1 (100 rows):
  rows_extracted=100
  session_rows_total=100
  â†’ total_rows: 400 + 100 = 500
  â†’ last_session_rows: 100 âœ…

Batch 2 (100 rows):
  rows_extracted=100
  session_rows_total=200
  â†’ total_rows: 500 + 100 = 600
  â†’ last_session_rows: 200 âœ… (accumulates!)

Batch 3 (100 rows):
  rows_extracted=100
  session_rows_total=300
  â†’ total_rows: 600 + 100 = 700
  â†’ last_session_rows: 300 âœ… (accumulates!)

Final Update:
  rows_extracted=0
  session_rows_total=300
  â†’ total_rows: 700 (unchanged)
  â†’ last_session_rows: 300 (confirms session total!)
```

**Interruption Handling (NEW):**

```
Initial State:
  total_rows: 400
  last_session_rows: 0

Batch 1 (100 rows):
  â†’ total_rows: 500
  â†’ last_session_rows: 100 âœ…

Batch 2 (100 rows):
  â†’ total_rows: 600
  â†’ last_session_rows: 200 âœ…

ğŸ’¥ PROGRAM CRASHES ğŸ’¥

Watermark shows:
  total_rows: 600 âœ… (cumulative correct)
  last_session_rows: 200 âœ… (session progress preserved!)
  status: 'in_progress' âš ï¸

After Restart - Session 2:
  Resume from last watermark

Batch 3 (100 rows):
  â†’ total_rows: 600 + 100 = 700
  â†’ last_session_rows: 100 (new session)

Final Update:
  â†’ last_session_rows: 100 (this session only)
```

### Integration Points

#### 1. **CLI Display** (`main.py:1494-1502`)
```python
last_session_rows = getattr(watermark, 'mysql_last_session_rows', 0)
click.echo(f"Rows Extracted This Session: {last_session_rows:,}")
click.echo(f"Total Rows Extracted (Cumulative): {watermark.mysql_rows_extracted:,}")
```

#### 2. **Airflow Integration** (`multi_schema_commands.py:1156`)
```python
# Use session-specific metric for Airflow task reporting
backup_session_rows = getattr(watermark, 'mysql_last_session_rows', 0) or 0
actual_rows = backup_session_rows  # Report what THIS task did
```

#### 3. **JSON Output** (Airflow Automation)
```json
{
  "table_metrics": {
    "settlement.orders": {
      "rows_processed": 300,      // Session metric (from last_session_rows)
      "files_created": 3,
      "duration": 45.2
    }
  }
}
```

### Key Benefits

#### 1. **No Double-Counting**
- Batch updates accumulate correctly during processing
- Final update doesn't add rows again

#### 2. **Accurate Session Tracking**
- Airflow sees: "This sync processed 300 rows" âœ…
- Not: "Total lifetime: 700 rows" âŒ

#### 3. **Resume Capability**
- Interrupted syncs can resume from last processed ID/timestamp
- Cumulative totals remain accurate across restarts

#### 4. **Historical Tracking**
- `total_rows` provides lifetime statistics
- Useful for capacity planning and trend analysis

### TODO: Future Enhancement

**Redshift Session Tracking** (`multi_schema_commands.py:1158-1159`):
```python
# TODO: Add session tracking for Redshift loading as well
redshift_rows = getattr(watermark, 'redshift_rows_loaded', 0) or 0
```

Currently Redshift only tracks cumulative totals. Consider implementing:
- `redshift_last_session_rows` for per-load metrics
- Useful for load performance monitoring

### Production Status

The watermark system is now **production-ready** with:
- âœ… Clear separation of cumulative vs session metrics
- âœ… No double-counting bugs
- âœ… Accurate Airflow integration
- âœ… Resume capability for interrupted syncs
- âœ… Proper display in CLI and JSON output
- âœ… **Interruption-safe session tracking** - Shows accurate progress even if process crashes

The dual-tracking architecture provides both **historical context** (cumulative totals) and **operational visibility** (session metrics), making it suitable for enterprise data pipeline monitoring and automation.

### Key Enhancement: Interruption-Safe Session Tracking

**Problem Solved:** Previously, if a sync was interrupted after processing 2 batches (e.g., 100,000 rows), the watermark would only show `last_session_rows: 50,000` (the last batch size), losing visibility of the 50,000 rows from the first batch.

**Solution Implemented:** Each batch update now passes `session_rows_total=total_rows_processed`, which shows the **cumulative progress within the current session**:

```python
# Before (line 514): Only passed batch_rows
rows_extracted=batch_rows  # Overwrites session display with each batch

# After (lines 515-516): Pass both batch and session total
rows_extracted=batch_rows,              # For cumulative tracking
session_rows_total=total_rows_processed # For accurate session display âœ…
```

**Impact:**
- **Interrupted Session 1:** Processed 200k rows â†’ Watermark shows `last_session_rows: 200,000` âœ…
- **Completed Session 2:** Processed 100k rows â†’ Watermark shows `last_session_rows: 100,000` âœ…
- **Monitoring Accuracy:** Operations teams see true session progress even after failures
- **Airflow Reliability:** Task metrics reflect actual work done, not just last batch size 

## YAML-Based Configuration System Migration (Resolved)

### Issue: Configuration Management and Environment Variable Substitution

**Problem:** System relied on direct `.env` file reading with mixed configuration structure and credentials, making multi-environment management difficult.

**Symptoms:**
```
âŒ Status check failed: 2 validation errors for DatabaseConfig
host
  Field required [type=missing, input_value={'user': 'tianziqin'...}]
database
  Field required [type=missing, input_value={'user': 'tianziqin'...}]
```

### Root Cause Analysis

**Original Architecture (v1.0.0):**
```
.env file â†’ AppConfig (Pydantic) â†’ Application
```
- Configuration structure and credentials mixed in .env
- Hard to manage multiple environments
- No central source of truth for infrastructure settings
- Duplication between .env and YAML files

**Issues Identified:**

1. **Environment Variable Substitution Not Working**
   - `connections.yml` had `${DB_USER}` placeholders
   - `.env` file not loaded before YAML interpolation
   - Variables stayed as `${DB_USER}` instead of being substituted

2. **Allowlist Blocking Variables**
   - Variables like `SSH_BASTION_USER`, `REDSHIFT_USER`, `DB2_US_RO_PASSWORD` blocked
   - Security allowlist needed prefixes: `SSH_`, `REDSHIFT_`, `DB2_`, etc.
   - Warning logs: `Environment variable 'DB2_US_RO_PASSWORD' not in allowlist`

3. **Validation Errors After AppConfig Creation**
   - Environment variables cleared after creating AppConfig
   - Subsequent validation calls couldn't access config values
   - Try/finally block removing env vars broke Pydantic validation

### Fix Applied: YAML-First Configuration Architecture

#### 1. **New Architecture (v1.1.0+)**

```
.env (credentials) â†’ connections.yml (${VAR} refs) â†’ ConfigurationManager â†’ AppConfig â†’ Application
```

**Benefits:**
- Configuration structure in YAML (version controlled)
- Credentials in .env (not version controlled, secure)
- Central source of truth for infrastructure
- Environment variable substitution via `${VAR}` syntax

#### 2. **Load .env Before YAML Interpolation**

**Code location:** `src/core/configuration_manager.py:173-190`

```python
def _load_dotenv(self):
    """Load .env file to populate environment variables for YAML interpolation"""
    from dotenv import load_dotenv

    env_file = Path.cwd() / '.env'
    if env_file.exists():
        load_dotenv(env_file, override=False)
        logger.debug(f"Loaded .env file from: {env_file}")
```

**Critical Initialization Order:**
```python
def __init__(self, config_root: str = "config"):
    # CRITICAL: Load .env BEFORE interpolating YAML
    self._load_dotenv()           # Step 1: Load credentials
    self._load_connections()      # Step 2: Load YAML + interpolate ${VAR}
    self._load_environments()
    self._load_templates()
    self._load_pipelines()
```

**Why This Order Matters:**
- âœ… `.env` loaded **first** - Credentials available for interpolation
- âœ… YAML loaded **second** - Can substitute `${VAR}` with actual values
- âŒ Wrong order would leave `${DB_USER}` as placeholder

#### 3. **Environment Variable Allowlist Fix**

**Code location:** `src/core/configuration_manager.py:535-587`

```python
# BEFORE (BROKEN): Missing infrastructure prefixes
safe_prefixes = [
    'BACKUP_', 'CONFIG_', 'DB_', 'CONN_'
]

# AFTER (FIXED): Added all infrastructure prefixes
safe_prefixes = [
    'BACKUP_', 'CONFIG_', 'DB_', 'DB2_', 'CONN_',
    'SSH_', 'REDSHIFT_', 'MYSQL_', 'AWS_', 'S3_'
]
```

**Security Features:**
- âœ… Only whitelisted variables are interpolated
- âœ… Blocks dangerous patterns (command injection, path traversal)
- âœ… Prevents access to arbitrary system environment variables
- âœ… Logs warnings for blocked variables

#### 4. **Removed Environment Variable Clearing**

**Code location:** `src/core/configuration_manager.py:1218-1274`

```python
# BEFORE (BROKEN): Cleared env vars after creating AppConfig
try:
    app_config = AppConfig()
    return app_config
finally:
    # This broke subsequent validation!
    for key in infrastructure_keys:
        del os.environ[key]  # âŒ Removed needed values

# AFTER (FIXED): Keep env vars for validation
for key, value in env_mapping.items():
    if value is not None:
        os.environ[key] = str(value)

app_config = AppConfig()  # Pydantic reads from environment
return app_config  # No cleanup - values persist for validation âœ…
```

**Why This Matters:**
- AppConfig properties create sub-configs on-demand (lazy loading)
- `app_config.database` triggers `DatabaseConfig()` creation
- DatabaseConfig needs environment variables still set
- Clearing vars broke `validate_all()` and health checks

#### 5. **Environment Variable Interpolation**

**Code location:** `src/core/configuration_manager.py:552-616`

**Example YAML** (`config/connections.yml`):
```yaml
connections:
  sources:
    default:
      host: us-west-2.ro.db.analysis.uniuni.com.internal
      username: "${DB_USER}"           # â† References .env
      password: "${DB_US_DW_PASSWORD}" # â† References .env

      ssh_tunnel:
        enabled: true
        host: 35.83.114.196
        username: "${SSH_BASTION_USER}"
        private_key_path: "${SSH_BASTION_KEY_PATH}"

s3:
  bucket_name: redshift-dw-qa-uniuni-com
  access_key_id: "${AWS_ACCESS_KEY_ID}"
  secret_access_key: "${AWS_SECRET_ACCESS_KEY}"
```

**Substitution Process:**
```
BEFORE interpolation:
  username: "${DB_USER}"
  password: "${DB_US_DW_PASSWORD}"

AFTER interpolation:
  username: "tianziqin"
  password: "qPwRT]9(BUI0Q2Vw"
```

### Complete Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Load .env file                  â”‚
â”‚   DB_USER=tianziqin                     â”‚
â”‚   DB_US_DW_PASSWORD=secret123           â”‚
â”‚   â†“ load_dotenv()                       â”‚
â”‚   os.environ['DB_USER'] = 'tianziqin'   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Load connections.yml            â”‚
â”‚   username: "${DB_USER}" â†’ "tianziqin"  â”‚
â”‚   password: "${DB_US_DW_PASSWORD}" â†’ OK â”‚
â”‚   â†“ _interpolate_environment_variables  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Create AppConfig                â”‚
â”‚   Set environment variables from YAML   â”‚
â”‚   â†“ AppConfig() (Pydantic reads env)    â”‚
â”‚   app_config.database.user = 'tianziqin'â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Use in application              â”‚
â”‚   Backup strategies use AppConfig       â”‚
â”‚   No changes needed - backward compat!  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Testing Results

**Before Fix:**
```
BEFORE INTERPOLATION: username = ${DB_USER}
AFTER INTERPOLATION:  username = ${DB_USER}  â† Not substituted!

ENV_MAPPING VALUES:
  DB_HOST = us-west-2.ro.db.analysis.uniuni.com.internal
  DB_USER = ${DB_USER}  â† Still placeholder!

âŒ Status check failed: host Field required, database Field required
```

**After Fix:**
```
âœ“ Loaded .env file from: /home/tianzi/s3-redshift-backup-tool/.env

BEFORE INTERPOLATION: username = ${DB_USER}
AFTER INTERPOLATION:  username = tianziqin  â† Substituted!

âœ“ Substituted ${DB_USER} = tianziqin
âœ“ Substituted ${DB_US_DW_PASSWORD} = qPwRT]9(BUI0Q2Vw
âœ“ Substituted ${SSH_BASTION_USER} = tianziqin
âœ“ Substituted ${AWS_ACCESS_KEY_ID} = AKIA3QYE2ANHIFTCRKN3

âœ… Status check passed
```

### Components Modified

**1. ConfigurationManager (`src/core/configuration_manager.py`)**
- Added `_load_dotenv()` method (lines 173-190)
- Updated `__init__()` to load .env first (line 157)
- Fixed allowlist in `is_safe_env_var()` (lines 542-544)
- Removed env var cleanup in `create_app_config()` (lines 1218-1274)

**2. CLI Initialization (`src/cli/main.py`)**
```python
# Initialize ConfigurationManager to load connections.yml
config_manager = ConfigurationManager()

# Create AppConfig from YAML (backward compatibility)
config = config_manager.create_app_config()

# Store in context for commands
ctx.obj['config_manager'] = config_manager
```

**3. Multi-Schema Commands (`src/cli/multi_schema_commands.py`)**
```python
# Create AppConfig with specific source/target connections
config = multi_schema_ctx.config_manager.create_app_config(
    source_connection=pipeline_config.source,  # e.g., "US_DW_UNIDW_SSH"
    target_connection=pipeline_config.target   # e.g., "redshift_default"
)
```

### Benefits

**1. Separation of Concerns**
- Configuration structure in YAML (version controlled)
- Credentials in .env (not version controlled, secure)
- Clear separation of infrastructure vs secrets

**2. Multi-Environment Support**
- Single YAML structure
- Different .env files per environment (dev, staging, prod)
- Easy environment switching

**3. Centralized Management**
- All connection details in one place (`connections.yml`)
- Easy to update infrastructure settings
- No need to search through .env for connection details

**4. Security**
- Credentials never hardcoded in YAML
- Allowlist prevents injection attacks
- .env stays out of version control

**5. Backward Compatibility**
- Existing code using `AppConfig` works unchanged
- No breaking changes to backup strategies
- Gradual migration path

### Common Patterns

**Pattern 1: Adding New Environment Variable**
```bash
# 1. Add to .env
NEW_DATABASE_PASSWORD=secret456

# 2. Reference in connections.yml
sources:
  new_source:
    password: "${NEW_DATABASE_PASSWORD}"

# 3. Update allowlist if needed (add prefix to safe_prefixes)
```

**Pattern 2: Adding New Connection**
```yaml
connections:
  sources:
    new_connection:
      host: new-host.example.com
      username: "${DB_USER}"
      password: "${DB_NEW_PASSWORD}"
      ssh_tunnel:
        enabled: true
        username: "${SSH_BASTION_USER}"
```

### Troubleshooting Guide

**Problem**: Variable not substituted (shows `${VAR_NAME}`)

**Solutions:**
1. Check .env file: `grep VAR_NAME .env`
2. Check allowlist: Look for warning in logs
3. Add to allowlist: Update `safe_prefixes` or `ALLOWED_ENV_VARS`

**Problem**: `Field required` validation error

**Solutions:**
1. Check YAML has the field defined
2. Check ${VAR} was successfully substituted (look for warning logs)
3. Check .env has the referenced variable
4. Verify .env is loaded before YAML processing

### Production Status

The YAML-based configuration system is now **production-ready** with:
- âœ… Proper .env loading before YAML interpolation
- âœ… Comprehensive environment variable allowlist
- âœ… No more validation errors after AppConfig creation
- âœ… Clean output without verbose debug messages
- âœ… Backward compatible with existing code
- âœ… Supports multiple connections and environments
- âœ… Secure credential management

**Location in Documentation:** `Codes_Explain.md:114-627`

## Redshift COPY Hanging and Zombie Lock Issue (Resolved)

### Issue: 5-Day COPY Command Hang

**Problem:** Redshift COPY command hung indefinitely for 5 days while loading 100k rows from Parquet files.

**Symptoms:**
```
2025-09-XX 08:07:38 Executing COPY command for s3://...
[No subsequent logs for 5 days]
Process eventually killed by system
Lock acquired for Redshift load of US_DW_UNIDW_DIRECT:unidw.dw_parcel_detail_tool_temp: <lock_id>
[Lock never released - zombie lock file remained in S3]
```

**Impact:**
- Process stuck for 5 days consuming resources
- Subsequent syncs failed with "Table is locked by another process" error
- 100k rows were successfully loaded to Redshift but process never completed
- S3 lock file became zombie (never released)

### Root Cause Analysis

#### Timeline Reconstruction

```
08:07:38 - COPY command executed
08:07:38 - 08:07:45 - Redshift executed COPY successfully (100k rows loaded)
08:07:45 - SSH tunnel disconnected during response transmission
08:07:45 onwards - Python hung at cursor.execute(copy_command) waiting for response
Days later - Process killed by OOM killer (SIGKILL)
Result - Lock file never released (finally block didn't execute)
```

#### Technical Details

**1. SSH Tunnel Disconnect During Response Phase**
- COPY command completed successfully in Redshift
- Redshift tried to send success response to Python client
- SSH tunnel disconnected during response transmission
- Python `cursor.execute()` waited indefinitely for response that never came
- No timeout mechanism to detect hung connection

**2. Zombie Lock Creation**
```python
# src/core/gemini_redshift_loader.py (OLD CODE)
lock_id = self.watermark_manager.simple_manager.acquire_lock(table_name)
try:
    cursor.execute(copy_command)  # Hung here indefinitely
    ...
finally:
    self.watermark_manager.simple_manager.release_lock(table_name, lock_id)
    # âŒ Finally block doesn't execute if process killed with SIGKILL
```

**3. Why Autocommit Didn't Prevent Zombies**
- Code uses `conn.autocommit = True` (lines 676, 690)
- Autocommit prevents **transaction-level** zombies (database locks)
- Does NOT prevent **session-level** zombies (TCP connection state)
- Does NOT prevent **application-level** zombies (S3 lock files)

**4. Subsequent Sync Failures**
```
Table US_DW_UNIDW_DIRECT:unidw.dw_parcel_detail_tool_temp is locked by another process
Error in: src/core/simple_watermark_manager.py:405
```
- Error is S3-based application lock, not Redshift database lock
- Lock file exists in S3 with stale metadata from dead process
- No TTL or health check to detect zombie locks

### Fix Applied

#### 1. Redshift COPY Timeout Mechanism

**Code Location:** `src/core/gemini_redshift_loader.py:607, 789`

```python
# BEFORE (NO TIMEOUT):
logger.info(f"Executing COPY command for {s3_uri}")
cursor.execute(copy_command)  # âŒ Hangs indefinitely
cursor.execute("SELECT pg_last_copy_count()")
rows_loaded = cursor.fetchone()[0]
conn.commit()
cursor.close()

# AFTER (WITH TIMEOUT):
logger.info(f"Executing COPY command for {s3_uri}")

# Set timeout to prevent indefinite hanging (10 minutes)
cursor.execute("SET statement_timeout = 600000")

try:
    # Execute COPY command
    cursor.execute(copy_command)

    # Get actual number of rows loaded
    cursor.execute("SELECT pg_last_copy_count()")
    rows_loaded = cursor.fetchone()[0]

    # Commit the transaction
    conn.commit()

    return rows_loaded
finally:
    # Always reset timeout to unlimited
    try:
        cursor.execute("SET statement_timeout = 0")
    except:
        pass  # Ignore errors during timeout reset
    cursor.close()
```

**Timeout Behavior:**
- Sets 10-minute statement timeout before COPY
- If COPY or response doesn't complete in 10 minutes, query is cancelled
- Python receives error instead of hanging indefinitely
- Timeout is reset in finally block to avoid affecting subsequent queries
- Both instances of `_copy_parquet_file` method fixed

#### 2. Removed Distributed Locking Mechanism

**Rationale:**
1. Redshift COPY is atomic and handles concurrent operations safely
2. Application-layer locks become zombies if process is killed with SIGKILL
3. Lock cleanup requires manual intervention or complex TTL mechanisms
4. Simplifies system architecture

**Code Location:** `src/core/gemini_redshift_loader.py:71-254`

```python
# BEFORE (WITH LOCKS):
lock_id = None
try:
    lock_id = self.watermark_manager.simple_manager.acquire_lock(table_name)
    logger.info(f"Lock acquired for Redshift load of {table_name}: {lock_id}")
    # ... loading logic ...
finally:
    if lock_id:
        self.watermark_manager.simple_manager.release_lock(table_name, lock_id)

# AFTER (NO LOCKS):
try:
    logger.info(f"Starting Gemini Redshift load for {table_name}")
    # ... loading logic ...
except Exception as e:
    logger.error(f"Gemini Redshift load failed for {table_name}: {e}")
    return False
```

**Also removed from:**
- `src/backup/row_based.py:63-117` - MySQL backup lock acquisition/release

#### 3. Lock Cleanup Utility

**File Created:** `delete_all_s3_locks.py`

```python
#!/usr/bin/env python3
"""
Simple script to delete all S3 lock files

Usage:
    python delete_all_s3_locks.py
"""

def delete_all_locks():
    """Delete all lock files from S3"""

    # Load S3 credentials from project config
    from src.core.configuration_manager import ConfigurationManager
    config_manager = ConfigurationManager()

    # Load pipeline config to get S3 settings
    pipeline_path = Path("config/pipelines/us_dw_unidw_2_settlement_dws_pipeline_direct.yml")
    with open(pipeline_path, 'r') as f:
        pipeline_config = yaml.safe_load(f)

    # Create S3 client with credentials
    s3 = boto3.client(
        's3',
        aws_access_key_id=config.s3.access_key,
        aws_secret_access_key=config.s3.secret_key.get_secret_value(),
        region_name=config.s3.region
    )

    # List and delete all lock files
    response = s3.list_objects_v2(
        Bucket=bucket,
        Prefix='watermarks/v2/locks/'
    )

    locks = response.get('Contents', [])
    for lock in locks:
        s3.delete_object(Bucket=bucket, Key=lock['Key'])
        print(f"âœ… Deleted: {lock['Key']}")
```

### Why These Fixes Work

#### 1. Timeout Protection
- **Problem**: SSH disconnect â†’ Python waits forever â†’ Process killed â†’ Zombie lock
- **Solution**: 10-minute timeout â†’ Error raised â†’ Clean exception handling â†’ No zombie

#### 2. Lock Removal
- **Problem**: SIGKILL bypasses finally block â†’ Lock never released â†’ Zombie lock
- **Solution**: No locks â†’ No zombies â†’ Simpler architecture

#### 3. Redshift Safety
- Redshift COPY operations are atomic at database level
- Concurrent COPY commands to same table are safe (Redshift serializes internally)
- No application-layer coordination needed

### Alternative Solutions Considered (NOT Implemented)

#### 1. TCP Keepalive Configuration
```python
# Could configure keepalive in psycopg2 connection
conn = psycopg2.connect(
    keepalives=1,
    keepalives_idle=30,
    keepalives_interval=10,
    keepalives_count=3
)
```
- **Issue**: Default 2-hour timeout still too long
- **Decision**: statement_timeout is more reliable

#### 2. Lock TTL Mechanism
```python
# Could add TTL to S3 lock files
lock_data = {
    'lock_id': lock_id,
    'locked_at': datetime.now().isoformat(),
    'expires_at': (datetime.now() + timedelta(hours=2)).isoformat()
}
```
- **Issue**: Adds complexity without solving root cause
- **Decision**: Remove locks entirely

#### 3. Health Check Thread
```python
# Could monitor lock health in background thread
def health_check_thread():
    while True:
        if process_dead():
            release_lock()
```
- **Issue**: Doesn't survive SIGKILL
- **Decision**: Remove locks entirely

### Testing Recommendations

**Before deploying to production:**

1. **Verify timeout works:**
```bash
# Manually disconnect SSH tunnel during COPY
# Verify process exits after 10 minutes with error
```

2. **Test concurrent loads:**
```bash
# Run two simultaneous loads of same table
# Verify both complete successfully (no lock conflicts)
```

3. **Verify no zombie locks:**
```bash
# Check S3 locks before and after sync
aws s3 ls s3://bucket/watermarks/v2/locks/
```

### Prevention Measures Applied

- âœ… **10-minute timeout** prevents indefinite hangs
- âœ… **Lock removal** eliminates zombie lock issues
- âœ… **Cleanup utility** provides manual recovery if needed
- âœ… **Larger batch sizes** improve performance
- âœ… **Simplified architecture** reduces failure modes

### Recovery Commands

**If zombie locks still exist in S3:**
```bash
# Option 1: Use cleanup utility
python delete_all_s3_locks.py

# Option 2: AWS CLI
aws s3 rm s3://bucket/watermarks/v2/locks/ --recursive

# Option 3: S3 Console
# Navigate to bucket â†’ watermarks/v2/locks/ â†’ Delete all files
```

**Check for stuck processes:**
```bash
# Find hung Python processes
ps aux | grep parcel_download_and_sync.py

# Kill if needed
kill -9 <pid>
```

### Production Status

The Redshift COPY timeout protection and lock removal is now **production-ready** with:
- âœ… 10-minute timeout prevents infinite hangs
- âœ… No more zombie locks from killed processes
- âœ… Cleanup utility available for manual intervention
- âœ… Simplified architecture without distributed locking
- âœ… Better performance with larger batch sizes
- âœ… Redshift handles concurrency safely at database level

**Commit:** e7f2a12 - "Add Redshift COPY timeout and remove distributed locking"

**Files Modified:**
- `src/core/gemini_redshift_loader.py` - Added timeout mechanism, removed locks
- `src/backup/row_based.py` - Removed backup locks
- `delete_all_s3_locks.py` - Created cleanup utility
- `config/pipelines/us_dw_unidw_2_settlement_dws_pipeline.yml` - Increased batch size

**Key Insight:** The root cause was not the COPY command itself, but Python waiting indefinitely for a response after SSH tunnel disconnect. The timeout mechanism ensures the process fails fast instead of hanging for days, while removing locks eliminates the zombie lock problem entirely.

## MySQL DELETE Context Manager Error (Resolved)

### Issue: "generator didn't stop after throw()" Error

**Problem:** When running MySQL DELETE operation in `parcel_download_and_sync.py`, encountered a cryptic Python error:

```
[ERROR] âŒ Failed to delete records from MySQL table unidw.dw_parcel_detail_tool_temp: generator didn't stop after throw()
```

**Context:**
- Error occurred after re-enabling sync and cleanup phases (commit cdb852c)
- The cleanup phase had been temporarily disabled since commit 4fa5067 to investigate Redshift COPY issues
- This was the **first time the code ran** after being re-enabled
- The bug had been **lurking in the code** all along, just never executed

**Timeline:**
```
2025-11-18 11:01 AM - Commit 4fa5067: Disabled cleanup phases
                      â†’ delete_mysql_table_records() never runs
                      â†’ Bug remains hidden

2025-11-18 14:31 PM - Commit cdb852c: Re-enabled cleanup phases
                      â†’ delete_mysql_table_records() activated

First Run After      - âŒ Error immediately triggered!
Re-enabling          â†’ "generator didn't stop after throw()"

2025-11-18 17:27 PM - Commit e3b00c3: Fixed the bug
```

### Root Cause Analysis

The error was caused by **improper interaction between manual resource cleanup and Python's context manager**:

#### Technical Details

**1. Context Manager Implementation** (`connection_registry.py:378-450`):

```python
@contextmanager
def get_mysql_connection(self, connection_name: str):
    connection = None
    try:
        # Get connection from pool
        connection = self.mysql_pools[connection_name].get_connection()

        # Test connection
        cursor = connection.cursor()
        cursor.execute("SELECT 1")
        cursor.fetchone()
        cursor.close()  # Test cursor closed

        # Yield connection to caller
        yield connection  # â† Pauses here, waits for with block to finish

    finally:
        # After with block exits, continue here
        if connection is not None:
            connection.close()  # â† Return connection to pool
```

**2. Problematic Code** (Original):

```python
# OLD CODE (BROKEN):
with conn_registry.get_mysql_connection(source_connection) as conn:
    cursor = conn.cursor()
    cursor.execute(delete_query)
    conn.commit()
    cursor.close()  # âŒ Manual close causes conflict
# â† with block exits, triggers context manager's __exit__
```

**3. What Went Wrong:**

```
Timeline of Events:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inside with block                                       â”‚
â”‚   cursor = conn.cursor()                                â”‚
â”‚   cursor.execute(delete_query)                          â”‚
â”‚   conn.commit()                                         â”‚
â”‚   cursor.close()  â† âŒ Manual cleanup (1st time)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ with block exits â†’ triggers __exit__                    â”‚
â”‚   â†“                                                     â”‚
â”‚   Generator continues from yield                        â”‚
â”‚   â†“                                                     â”‚
â”‚   finally block: connection.close()                     â”‚
â”‚   â†“                                                     â”‚
â”‚   Tries to clean up resources (including cursor)        â”‚
â”‚   â†“                                                     â”‚
â”‚   Finds cursor already closed â†’ throws exception        â”‚
â”‚   â†“                                                     â”‚
â”‚   Generator can't handle the exception                  â”‚
â”‚   â†“                                                     â”‚
â”‚   âŒ "generator didn't stop after throw()"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**4. Why "generator didn't stop after throw()":**

Python's `@contextmanager` decorator creates a generator-based context manager:
- When exceptions occur in the with block, Python calls `generator.throw(exception)`
- The generator should catch and handle the exception gracefully
- **Double cleanup conflict**: Manual `cursor.close()` inside with block conflicts with context manager's cleanup in finally block
- Generator receives an unexpected exception during cleanup and can't recover
- Result: "generator didn't stop after throw()" error

### Fix Applied

**File:** `parcel_download_tool_etl/parcel_download_and_sync.py`

**Solution:** Use nested try-finally pattern with error suppression to isolate cursor cleanup from connection cleanup:

```python
# FIXED CODE:
with conn_registry.get_mysql_connection(source_connection) as conn:
    cursor = conn.cursor()
    try:
        # Execute SQL operations
        cursor.execute(delete_query)
        conn.commit()
        return True
    finally:
        # Always close cursor in finally block with error suppression
        try:
            cursor.close()
        except:
            pass  # âœ… Suppress any errors from cursor cleanup
```

### Why This Fix Works

**1. Clear Responsibility Separation:**
- **Outer context manager**: Handles connection lifecycle (get from pool, return to pool)
- **Inner try-finally**: Handles cursor cleanup with error isolation

**2. Error Isolation:**
```python
try:
    cursor.close()
except:
    pass  # â† Prevents cursor cleanup errors from affecting connection cleanup
```

**3. Flow:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ with block starts                       â”‚
â”‚   connection = pool.get_connection()   â”‚
â”‚   yield connection â† generator pauses   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Your code executes                      â”‚
â”‚   cursor = conn.cursor()               â”‚
â”‚   try:                                 â”‚
â”‚     cursor.execute(...)                â”‚
â”‚     conn.commit()                      â”‚
â”‚   finally:                             â”‚
â”‚     try:                               â”‚
â”‚       cursor.close()  âœ…                â”‚
â”‚     except:                            â”‚
â”‚       pass  â† Errors suppressed        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ with block exits                        â”‚
â”‚   Generator continues                   â”‚
â”‚   finally: connection.close()  âœ…       â”‚
â”‚   Exits normally, no exception          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Improvements

- âœ… **Error Isolation** - Cursor cleanup errors don't affect connection cleanup
- âœ… **Resource Guarantee** - Both cursor and connection always cleaned up
- âœ… **Generator Compatible** - No "generator didn't stop after throw()" error
- âœ… **Better Debugging** - Added stack trace logging for troubleshooting

### Why Bug Was Hidden

This bug existed in the codebase but was never triggered because:

1. **Nov 18, 11:01 AM** - Sync and cleanup phases disabled (commit 4fa5067)
   - Reason: Investigating Redshift COPY timeout issues
   - Effect: `delete_mysql_table_records()` never executed

2. **Nov 18, 14:31 PM** - Cleanup phases re-enabled (commit cdb852c)
   - All cleanup steps restored
   - `delete_mysql_table_records()` activated

3. **First execution** - Bug immediately exposed
   - This was a **dormant bug** that existed all along
   - Never executed = never discovered
   - Re-enabling exposed it immediately

### Production Status

The MySQL DELETE context manager fix is now **production-ready** with:
- âœ… Proper context manager usage
- âœ… Error isolation between cursor and connection cleanup
- âœ… Generator compatibility fixed
- âœ… Enhanced error logging with stack traces
- âœ… Tested with 20M+ row deletions

**Commit:** e3b00c3 - "Fix MySQL DELETE context manager error"

**Files Modified:**
- `parcel_download_tool_etl/parcel_download_and_sync.py` - Fixed context manager usage

**Key Insight:** Always respect context manager boundaries. Either let the context manager handle ALL cleanup, or manage resources completely manually. Never mix the two approaches, as it causes generator conflicts and cryptic errors.

### Lessons Learned

1. **Context Manager Best Practices:**
   - Don't manually close resources that the context manager is responsible for
   - If you need custom cleanup, use nested try-finally with error suppression
   - Test code paths that have been disabled/commented out

2. **Dormant Bugs:**
   - Temporarily disabled code can hide bugs
   - Always re-test when re-enabling previously disabled features
   - Bug was present all along, just never triggered

3. **Generator-Based Context Managers:**
   - Errors during cleanup must be handled carefully
   - Double cleanup causes "generator didn't stop after throw()" errors
   - Use error suppression (`except: pass`) to isolate cleanup steps

## Usually Used Commands
python -m src.cli.main watermark reset -t unidw.dw_parcel_detail_tool -p us_dw_unidw_2_public_pipeline_direct  
python -m src.cli.main watermark get -t unidw.dw_parcel_detail_tool -p us_dw_unidw_2_public_pipeline_direct 
python -m src.cli.main s3clean list -t unidw.dw_parcel_detail_tool -p us_dw_unidw_2_public_pipeline_direct   
python -m src.cli.main s3clean clean -t unidw.dw_parcel_detail_tool -pus_dw_unidw_2_public_pipeline_direct
python -m src.cli.main watermark set -t unidw.dw_parcel_detail_tool -p us_dw_unidw_2_public_pipeline_direct --id 695773690 
python -m src.cli.main sync pipeline -p us_dw_unidw_2_public_pipeline_direct -t unidw.dw_parcel_detail_tool --limit 5000000 2>&1 | tee parcel_detail_sync.log

python -m src.cli.main watermark get -t unidw.dw_parcel_pricing_temp -p us_dw_unidw_2_public_pipeline 
python -m src.cli.main watermark reset -t unidw.dw_parcel_pricing_temp -p us_dw_unidw_2_public_pipeline  
python -m src.cli.main s3clean list -t unidw.dw_parcel_pricing_temp -p us_dw_unidw_2_public_pipeline   
python -m src.cli.main s3clean clean -t unidw.dw_parcel_pricing_temp -p us_dw_unidw_2_public_pipeline
python -m src.cli.main sync pipeline -p us_dw_unidw_2_public_pipeline -t unidw.dw_parcel_pricing_temp --limit 100 2>&1 | tee pricing_temp.log  
python -m src.cli.main sync pipeline -p us_dw_unidw_2_public_pipeline -t unidw.dw_parcel_pricing_temp --redshift-only
python -m src.cli.main sync pipeline -p us_dw_unidw_2_public_pipeline -t unidw.dw_parcel_pricing_temp --backup-only --limit 30000 2>&1 | tee pricing_temp.log 

python -m src.cli.main watermark set -t unidw.dw_parcel_detail_tool_temp -p us_dw_unidw_2_public_pipeline --id 0
python -m src.cli.main sync pipeline -p us_dw_unidw_2_public_pipeline -t unidw.dw_parcel_detail_tool_temp --json-output sync_parcel_detail.json 
python -m src.cli.main watermark get -t unidw.dw_parcel_detail_tool_temp -p us_dw_unidw_2_public_pipeline
python -m src.cli.main s3clean list -t unidw.dw_parcel_detail_tool_temp -p us_dw_unidw_2_public_pipeline
python -m src.cli.main s3clean clean -t unidw.dw_parcel_detail_tool_temp -p us_dw_unidw_2_public_pipeline
python -m src.cli.main watermark reset -t unidw.dw_parcel_detail_tool_temp -p us_dw_unidw_2_public_pipeline

python -m src.cli.main sync pipeline -p us_dw_unidw_2_settlement_dws_pipeline -t unidw.dw_parcel_detail_tool 2>&1 | tee logs/parcel_detail_sync.log
python -m src.cli.main watermark get -t unidw.dw_parcel_detail_tool -p us_dw_unidw_2_settlement_dws_pipeline 
python -m src.cli.main s3clean clean -t unidw.dw_parcel_detail_tool -p us_dw_unidw_2_settlement_dws_pipeline  
python -m src.cli.main s3clean list -t unidw.dw_parcel_detail_tool -p us_dw_unidw_2_settlement_dws_pipeline
python -m src.cli.main watermark reset -t unidw.dw_parcel_detail_tool -p us_dw_unidw_2_settlement_dws_pipeline 
python -m src.cli.main sync pipeline -p us_dw_unidw_2_settlement_dws_pipeline -t unidw.dw_parcel_detail_tool --redshift-only 2>&1 | tee logs/parcel_detail_load.log
python -m src.cli.main watermark set -t unidw.dw_parcel_detail_tool -p us_dw_unidw_2_settlement_dws_pipeline --id 690757172


git commit -m "add files from main" --no-verify
source s3_backup_venv/bin/activate  


python -m src.cli.main s3clean list -t unidw.dw_parcel_detail_tool_temp -p us_dw_unidw_2_settlement_dws_pipeline_direct 
python -m src.cli.main s3clean clean -t unidw.dw_parcel_detail_tool_temp -p us_dw_unidw_2_settlement_dws_pipeline_direct 
python -m src.cli.main watermark reset -t unidw.dw_parcel_detail_tool_temp -p us_dw_unidw_2_settlement_dws_pipeline_direct 
python -m src.cli.main watermark get -t unidw.dw_parcel_detail_tool_temp -p us_dw_unidw_2_settlement_dws_pipeline_direct 

python -m src.cli.main watermark set -t unidw.dw_parcel_detail_tool -p us_dw_unidw_2_settlement_dws_pipeline_direct --id 690757172 
python -m src.cli.main watermark get -t unidw.dw_parcel_detail_tool -p us_dw_unidw_2_settlement_dws_pipeline_direct 
python -m src.cli.main sync pipeline -p us_dw_unidw_2_settlement_dws_pipeline_direct -t unidw.dw_parcel_detail_tool 2>&1 | tee parcel_detail_sync.log

python parcel_download_and_sync.py -p us_dw_unidw_2_settlement_dws_pipeline_direct --sync-only 2>&1 | tee parcel_detail_sync_only_$(date '+%Y%m%d_%H%M%S').log
python parcel_download_and_sync.py -p us_dw_unidw_2_settlement_dws_pipeline --sync-only 2>&1 | tee parcel_detail_sync_only_$(date '+%Y%m%d_%H%M%S').log