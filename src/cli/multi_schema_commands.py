"""
Multi-Schema CLI Commands for v1.1.0

Enhanced CLI commands supporting multi-connection data integration while maintaining
full backward compatibility with v1.0.0 syntax and workflows.
"""

import click
from typing import List, Optional, Dict, Any
from pathlib import Path
import sys
import os
from dataclasses import asdict

from src.core.connection_registry import ConnectionRegistry
from src.core.configuration_manager import ConfigurationManager
from src.utils.exceptions import ValidationError, ConnectionError
from src.utils.logging import get_logger

# Airflow Integration
from src.cli.airflow_integration import enhance_sync_with_airflow_integration

logger = get_logger(__name__)


class MultiSchemaContext:
    """Context object for multi-schema CLI commands"""
    
    def __init__(self):
        self.connection_registry: Optional[ConnectionRegistry] = None
        self.config_manager: Optional[ConfigurationManager] = None
        self._initialized = False
    
    def ensure_initialized(self, config_root: str = "config"):
        """Ensure multi-schema components are initialized"""
        if self._initialized:
            return
        
        try:
            self.connection_registry = ConnectionRegistry()
            self.config_manager = ConfigurationManager(config_root)
            self._initialized = True
            
            logger.info("Multi-schema CLI context initialized")
            
        except Exception as e:
            logger.error(f"Failed to initialize multi-schema context: {e}")
            click.echo(f"‚ùå Initialization failed: {e}")
            click.echo("üí° Tip: Run 'python -m src.cli.main config setup' to create default configuration")
            sys.exit(1)
    
    def is_v1_0_0_mode(self) -> bool:
        """Check if we should operate in v1.0.0 compatibility mode"""
        # Check if multi-schema configuration exists
        config_path = Path("config/connections.yml")
        if not config_path.exists():
            return True
        
        # Check if only default connections are configured
        try:
            if self.connection_registry:
                connections = self.connection_registry.list_connections()
                return set(connections.keys()) == {"default"}
        except:
            return True
        
        return False


# Global context
multi_schema_ctx = MultiSchemaContext()


def add_multi_schema_commands(cli):
    """Add multi-schema commands to the main CLI"""
    
    # Enhanced sync command group
    @cli.group(name="sync", invoke_without_command=True)
    @click.option('--table', '-t', multiple=True, help='Table names (v1.0.0 compatibility)')
    @click.option('--backup-only', is_flag=True, help='Only backup to S3')
    @click.option('--redshift-only', is_flag=True, help='Only load to Redshift')
    @click.option('--limit', type=int, help='Limit rows per query (testing)')
    @click.option('--max-workers', type=int, help='Maximum parallel workers (overrides config)')
    @click.option('--max-chunks', type=int, help='Maximum number of chunks to process (total rows = limit √ó max-chunks)')
    @click.pass_context
    def sync_command(ctx, table: List[str], backup_only: bool, redshift_only: bool, limit: Optional[int], max_workers: Optional[int], max_chunks: Optional[int]):
        """Sync data with multi-schema support and v1.0.0 compatibility"""
        
        if ctx.invoked_subcommand is None:
            # Handle direct sync command (v1.0.0 compatibility)
            if table:
                multi_schema_ctx.ensure_initialized()
                
                if multi_schema_ctx.is_v1_0_0_mode():
                    # Use v1.0.0 compatibility mode
                    _sync_legacy_mode(table, backup_only, redshift_only, limit, max_workers, max_chunks)
                else:
                    # Use default pipeline
                    _sync_with_default_pipeline(table, backup_only, redshift_only, limit, max_workers, max_chunks)
            else:
                # Show help for new syntax options
                _show_sync_help()
    
    @sync_command.command(name="pipeline")
    @click.option('--pipeline', '-p', required=True, help='Pipeline configuration name')
    @click.option('--table', '-t', multiple=True, required=True, help='Table names to sync')
    @click.option('--backup-only', is_flag=True, help='Only backup to S3 (MySQL ‚Üí S3)')
    @click.option('--redshift-only', is_flag=True, help='Only load to Redshift (S3 ‚Üí Redshift)')
    @click.option('--limit', type=int, help='Limit rows per query (testing)')
    @click.option('--max-workers', type=int, help='Maximum parallel workers (overrides config)')
    @click.option('--max-chunks', type=int, help='Maximum number of chunks to process (total rows = limit √ó max-chunks)')
    @click.option('--dry-run', is_flag=True, help='Preview operations without execution')
    @click.option('--parallel', is_flag=True, help='Override pipeline strategy to use parallel processing')
    @click.option('--json-output', type=click.Path(), help='Output execution metadata as JSON file for Airflow monitoring')
    @click.option('--s3-completion-bucket', type=str, help='S3 bucket for Airflow completion markers')
    def sync_pipeline(pipeline: str, table: List[str], backup_only: bool, redshift_only: bool, 
                     limit: Optional[int], max_workers: Optional[int], max_chunks: Optional[int], dry_run: bool, parallel: bool,
                     json_output: Optional[str] = None, s3_completion_bucket: Optional[str] = None):
        """Sync tables using pipeline configuration (v1.1.0 enhanced syntax)"""
        
        multi_schema_ctx.ensure_initialized()
        
        try:
            # Load and validate pipeline configuration
            pipeline_config = multi_schema_ctx.config_manager.get_pipeline_config(pipeline)
            
            # Validate pipeline
            validation_report = multi_schema_ctx.config_manager.validate_pipeline(pipeline)
            if not validation_report['valid']:
                click.echo(f"‚ùå Pipeline validation failed:")
                for error in validation_report['errors'][:5]:  # Show first 5 errors
                    click.echo(f"  ‚Ä¢ {error}")
                if len(validation_report['errors']) > 5:
                    click.echo(f"  ... and {len(validation_report['errors']) - 5} more errors")
                return
            
            # Show pipeline information
            click.echo(f"üöÄ Pipeline: {pipeline_config.name} (v{pipeline_config.version})")
            click.echo(f"üìù {pipeline_config.description}")
            click.echo(f"üîó {pipeline_config.source} ‚Üí {pipeline_config.target}")
            click.echo(f"üìã Tables: {', '.join(table)}")
            
            if dry_run:
                click.echo("üîç DRY RUN - Preview mode enabled")
            
            if parallel and pipeline_config.processing.get('strategy') != 'parallel':
                click.echo("‚ö° Overriding to parallel processing")
            
            # Check if Airflow integration is requested
            if json_output or s3_completion_bucket:
                click.echo("üöÄ Using Airflow integration features")
                
                # Create sync executor function for Airflow integration
                def execute_sync_tables(tracker):
                    """Execute sync for all tables and return results"""
                    table_results = {}
                    success_count = 0
                    
                    for table_name in table:
                        if table_name not in pipeline_config.tables:
                            click.echo(f"‚ö†Ô∏è  Warning: Table {table_name} not configured in pipeline")
                            
                            # For default pipeline, allow dynamic registration
                            if pipeline == "default":
                                multi_schema_ctx.config_manager.register_table_dynamically(pipeline, table_name)
                                click.echo(f"üìù Registered {table_name} dynamically in default pipeline")
                            else:
                                table_results[table_name] = {
                                    "success": False,
                                    "error_message": f"Table {table_name} not configured in pipeline"
                                }
                                continue
                        
                        table_config = pipeline_config.tables[table_name]
                        
                        if dry_run:
                            _preview_table_sync(pipeline_config, table_config, backup_only, redshift_only)
                            table_results[table_name] = {
                                "success": True,
                                "rows_processed": 0,  # Dry run
                                "files_created": 0,
                                "duration": 0
                            }
                            success_count += 1
                        else:
                            result = _execute_table_sync(
                                pipeline_config, table_config, 
                                backup_only, redshift_only, limit, parallel, max_workers, max_chunks
                            )
                            
                            # FIXED: Use actual metrics from _execute_table_sync instead of hardcoded values
                            table_results[table_name] = result
                            
                            if result.get("success", False):
                                success_count += 1
                    
                    return success_count, table_results
                
                # Use Airflow integration
                exit_code = enhance_sync_with_airflow_integration(
                    pipeline=pipeline,
                    tables=list(table),
                    sync_executor_func=execute_sync_tables,
                    json_output_path=json_output,
                    s3_completion_bucket=s3_completion_bucket,
                    completion_prefix="completion_markers"
                )
                sys.exit(exit_code)
            
            else:
                # Original logic for backward compatibility
                success_count = 0
                for table_name in table:
                    if table_name not in pipeline_config.tables:
                        click.echo(f"‚ö†Ô∏è  Warning: Table {table_name} not configured in pipeline")
                        
                        # For default pipeline, allow dynamic registration
                        if pipeline == "default":
                            multi_schema_ctx.config_manager.register_table_dynamically(pipeline, table_name)
                            click.echo(f"üìù Registered {table_name} dynamically in default pipeline")
                        else:
                            click.echo(f"‚ùå Skipping unconfigured table: {table_name}")
                            continue
                    
                    table_config = pipeline_config.tables[table_name]
                    
                    if dry_run:
                        _preview_table_sync(pipeline_config, table_config, backup_only, redshift_only)
                        success_count += 1
                    else:
                        success = _execute_table_sync(
                            pipeline_config, table_config, 
                            backup_only, redshift_only, limit, parallel, max_workers, max_chunks
                        )
                        if success:
                            success_count += 1
                
                # Summary
                total_tables = len(table)
                if success_count == total_tables:
                    click.echo(f"‚úÖ Pipeline completed successfully: {success_count}/{total_tables} tables")
                else:
                    click.echo(f"‚ö†Ô∏è  Pipeline completed with issues: {success_count}/{total_tables} tables successful")
                    if success_count < total_tables:
                        sys.exit(1)
        
        except Exception as e:
            logger.error(f"Pipeline sync failed: {e}")
            click.echo(f"‚ùå Pipeline sync failed: {e}")
            sys.exit(1)
    
    @sync_command.command(name="connections")
    @click.option('--source', '-s', required=True, help='Source connection name')
    @click.option('--target', '-r', required=True, help='Target connection name')
    @click.option('--table', '-t', multiple=True, required=True, help='Table names to sync')
    @click.option('--backup-only', is_flag=True, help='Only backup to S3')
    @click.option('--redshift-only', is_flag=True, help='Only load to Redshift')
    @click.option('--limit', type=int, help='Limit rows per query')
    @click.option('--max-workers', type=int, help='Maximum parallel workers (overrides config)')
    @click.option('--max-chunks', type=int, help='Maximum number of chunks to process (total rows = limit √ó max-chunks)')
    @click.option('--batch-size', type=int, default=10000, help='Batch size for processing')
    def sync_connections(source: str, target: str, table: List[str], backup_only: bool, 
                        redshift_only: bool, limit: Optional[int], max_workers: Optional[int], max_chunks: Optional[int], batch_size: int):
        """Sync tables using explicit connections (v1.1.0 ad-hoc syntax)"""
        
        multi_schema_ctx.ensure_initialized()
        
        try:
            # Validate connections exist
            source_info = multi_schema_ctx.connection_registry.get_connection_info(source)
            target_info = multi_schema_ctx.connection_registry.get_connection_info(target)
            
            click.echo(f"üöÄ Ad-hoc Connection Sync")
            click.echo(f"üìä Source: {source} ({source_info['database']}@{source_info['host']})")
            click.echo(f"üéØ Target: {target} ({target_info['database']}@{target_info['host']})")
            click.echo(f"üìã Tables: {', '.join(table)}")
            
            # Test connections
            click.echo("üîç Testing connections...")
            
            source_test = multi_schema_ctx.connection_registry.test_connection(source)
            if not source_test['success']:
                click.echo(f"‚ùå Source connection failed: {source_test['error']}")
                return
            click.echo(f"‚úÖ Source connection: {source} ({source_test['duration_seconds']}s)")
            
            target_test = multi_schema_ctx.connection_registry.test_connection(target)
            if not target_test['success']:
                click.echo(f"‚ùå Target connection failed: {target_test['error']}")
                return
            click.echo(f"‚úÖ Target connection: {target} ({target_test['duration_seconds']}s)")
            
            # Create ad-hoc pipeline configuration
            ad_hoc_pipeline = _create_adhoc_pipeline_config(source, target, list(table), batch_size)
            
            # Process tables
            success_count = 0
            for table_name in table:
                table_config = ad_hoc_pipeline.tables[table_name]
                success = _execute_table_sync(
                    ad_hoc_pipeline, table_config,
                    backup_only, redshift_only, limit, False, max_workers, max_chunks
                )
                if success:
                    success_count += 1
            
            # Summary
            total_tables = len(table)
            if success_count == total_tables:
                click.echo(f"‚úÖ Connection sync completed: {success_count}/{total_tables} tables")
            else:
                click.echo(f"‚ö†Ô∏è  Connection sync completed with issues: {success_count}/{total_tables} tables")
                if success_count < total_tables:
                    sys.exit(1)
        
        except Exception as e:
            logger.error(f"Connection-based sync failed: {e}")
            click.echo(f"‚ùå Connection sync failed: {e}")
            sys.exit(1)
    
    # Configuration management commands
    @cli.group(name="config")
    def config_command():
        """Configuration management for multi-schema support"""
        pass
    
    @config_command.command(name="setup")
    @click.option('--force', is_flag=True, help='Overwrite existing configuration')
    def config_setup(force: bool):
        """Set up default configuration for v1.1.0 multi-schema support"""
        
        config_dir = Path("config")
        
        # Check if configuration already exists
        if config_dir.exists() and not force:
            existing_files = list(config_dir.glob("*.yml"))
            if existing_files:
                click.echo("‚ö†Ô∏è  Configuration directory already exists with files:")
                for file in existing_files:
                    click.echo(f"  ‚Ä¢ {file}")
                click.echo("Use --force to overwrite existing configuration")
                return
        
        try:
            # Initialize configuration manager (will create default files)
            multi_schema_ctx.ensure_initialized()
            
            click.echo("‚úÖ Configuration setup completed!")
            click.echo("")
            click.echo("üìÅ Created configuration structure:")
            click.echo("  config/")
            click.echo("    ‚îú‚îÄ‚îÄ connections.yml       # Database connections")
            click.echo("    ‚îú‚îÄ‚îÄ pipelines/")
            click.echo("    ‚îÇ   ‚îî‚îÄ‚îÄ default.yml      # v1.0.0 compatibility pipeline") 
            click.echo("    ‚îî‚îÄ‚îÄ environments/")
            click.echo("        ‚îú‚îÄ‚îÄ development.yml   # Development settings")
            click.echo("        ‚îî‚îÄ‚îÄ production.yml    # Production settings")
            click.echo("")
            click.echo("üîß Next steps:")
            click.echo("  1. Review and update config/connections.yml with your database details")
            click.echo("  2. Create additional pipelines in config/pipelines/")
            click.echo("  3. Test connections: python -m src.cli.main connections test default")
            
        except Exception as e:
            click.echo(f"‚ùå Configuration setup failed: {e}")
            sys.exit(1)
    
    @config_command.command(name="list-pipelines")
    def config_list_pipelines():
        """List available pipeline configurations"""
        
        multi_schema_ctx.ensure_initialized()
        
        pipelines = multi_schema_ctx.config_manager.list_pipelines()
        
        if not pipelines:
            click.echo("üìã No pipeline configurations found")
            click.echo("üí° Run 'python -m src.cli.main config setup' to create default configuration")
            return
        
        click.echo("üìã Available Pipeline Configurations:")
        click.echo("")
        
        for pipeline_name in pipelines:
            try:
                pipeline_config = multi_schema_ctx.config_manager.get_pipeline_config(pipeline_name)
                
                # Get validation status
                validation = multi_schema_ctx.config_manager.validate_pipeline(pipeline_name)
                status_icon = "‚úÖ" if validation['valid'] else "‚ùå"
                
                click.echo(f"  {status_icon} {pipeline_name}")
                click.echo(f"      Name: {pipeline_config.name}")
                click.echo(f"      Version: {pipeline_config.version}")
                click.echo(f"      Description: {pipeline_config.description}")
                click.echo(f"      Route: {pipeline_config.source} ‚Üí {pipeline_config.target}")
                click.echo(f"      Tables: {len(pipeline_config.tables)}")
                
                if not validation['valid']:
                    click.echo(f"      Issues: {len(validation['errors'])} error(s)")
                
                click.echo("")
                
            except Exception as e:
                click.echo(f"  ‚ùå {pipeline_name} (Error: {e})")
                click.echo("")
    
    @config_command.command(name="show-pipeline")
    @click.argument('pipeline')
    @click.option('--verbose', '-v', is_flag=True, help='Show detailed configuration')
    def config_show_pipeline(pipeline: str, verbose: bool):
        """Show detailed pipeline configuration"""
        
        multi_schema_ctx.ensure_initialized()
        
        try:
            pipeline_config = multi_schema_ctx.config_manager.get_pipeline_config(pipeline)
            
            click.echo(f"üìä Pipeline Configuration: {pipeline}")
            click.echo("=" * 50)
            click.echo(f"Name: {pipeline_config.name}")
            click.echo(f"Version: {pipeline_config.version}")
            click.echo(f"Description: {pipeline_config.description}")
            click.echo(f"Source: {pipeline_config.source}")
            click.echo(f"Target: {pipeline_config.target}")
            click.echo("")
            
            # Processing configuration
            click.echo("üîß Processing Configuration:")
            for key, value in pipeline_config.processing.items():
                click.echo(f"  {key}: {value}")
            click.echo("")
            
            # S3 configuration
            click.echo("üóÇÔ∏è  S3 Configuration:")
            for key, value in pipeline_config.s3.items():
                click.echo(f"  {key}: {value}")
            click.echo("")
            
            # Tables
            click.echo(f"üìã Tables ({len(pipeline_config.tables)}):")
            
            for table_name, table_config in pipeline_config.tables.items():
                click.echo(f"  ‚Ä¢ {table_name}")
                click.echo(f"    Full name: {table_config.full_name}")
                click.echo(f"    Target: {table_config.target_name}")
                click.echo(f"    Type: {table_config.table_type}")
                click.echo(f"    CDC Strategy: {table_config.cdc_strategy}")
                
                if table_config.depends_on:
                    click.echo(f"    Dependencies: {', '.join(table_config.depends_on)}")
                
                if verbose:
                    click.echo(f"    Timestamp Column: {table_config.cdc_timestamp_column}")
                    click.echo(f"    ID Column: {table_config.cdc_id_column}")
                    
                    if table_config.processing:
                        click.echo("    Processing:")
                        for key, value in table_config.processing.items():
                            click.echo(f"      {key}: {value}")
                    
                    if table_config.validation:
                        click.echo("    Validation:")
                        for key, value in table_config.validation.items():
                            click.echo(f"      {key}: {value}")
                
                click.echo("")
        
        except Exception as e:
            click.echo(f"‚ùå Error showing pipeline: {e}")
            sys.exit(1)
    
    @config_command.command(name="validate-pipeline")
    @click.argument('pipeline')
    @click.option('--verbose', '-v', is_flag=True, help='Show detailed validation results')
    def config_validate_pipeline(pipeline: str, verbose: bool):
        """Validate pipeline configuration"""
        
        multi_schema_ctx.ensure_initialized()
        
        try:
            validation_report = multi_schema_ctx.config_manager.validate_pipeline(pipeline)
            
            if validation_report['valid']:
                click.echo(f"‚úÖ Pipeline '{pipeline}' is valid")
                click.echo(f"üìä Tables validated: {validation_report['tables_validated']}")
                
                if verbose and validation_report['warnings']:
                    click.echo("")
                    click.echo("‚ö†Ô∏è  Warnings:")
                    for warning in validation_report['warnings']:
                        click.echo(f"  ‚Ä¢ {warning}")
            else:
                click.echo(f"‚ùå Pipeline '{pipeline}' has validation errors:")
                click.echo("")
                
                # Show errors by category
                details = validation_report['validation_details']
                
                if not details['pipeline_structure']['valid']:
                    click.echo("üèóÔ∏è  Pipeline Structure Issues:")
                    for issue in details['pipeline_structure']['issues']:
                        click.echo(f"  ‚Ä¢ {issue}")
                    click.echo("")
                
                if not details['table_configurations']['valid']:
                    click.echo("üìã Table Configuration Issues:")
                    for issue in details['table_configurations']['issues']:
                        click.echo(f"  ‚Ä¢ {issue}")
                    click.echo("")
                
                if not details['dependencies']['valid']:
                    click.echo("üîó Dependency Issues:")
                    for issue in details['dependencies']['issues']:
                        click.echo(f"  ‚Ä¢ {issue}")
                    click.echo("")
                
                if not details['resource_requirements']['valid']:
                    click.echo("‚ö° Resource Requirement Issues:")
                    for issue in details['resource_requirements']['issues']:
                        click.echo(f"  ‚Ä¢ {issue}")
                    click.echo("")
                
                # Show warnings if verbose
                if verbose and validation_report['warnings']:
                    click.echo("‚ö†Ô∏è  Warnings:")
                    for warning in validation_report['warnings']:
                        click.echo(f"  ‚Ä¢ {warning}")
                
                sys.exit(1)
        
        except Exception as e:
            click.echo(f"‚ùå Validation failed: {e}")
            sys.exit(1)
    
    @config_command.command(name="status")
    def config_status():
        """Show overall configuration status"""
        
        multi_schema_ctx.ensure_initialized()
        
        status = multi_schema_ctx.config_manager.get_configuration_status()
        
        click.echo("üìä Configuration Status")
        click.echo("=" * 30)
        
        # Pipelines
        pipelines = status['pipelines']
        click.echo(f"üìã Pipelines: {pipelines['count']}")
        if pipelines['names']:
            for name in pipelines['names']:
                click.echo(f"  ‚Ä¢ {name}")
        click.echo("")
        
        # Environments
        environments = status['environments']
        click.echo(f"üåç Environments: {environments['count']}")
        click.echo(f"  Current: {environments['current']}")
        if environments['available']:
            click.echo(f"  Available: {', '.join(environments['available'])}")
        click.echo("")
        
        # Templates
        templates = status['templates']
        if templates['count'] > 0:
            click.echo(f"üìÑ Templates: {templates['count']}")
            if templates['available']:
                click.echo(f"  Available: {', '.join(templates['available'])}")
            click.echo("")
        
        # Configuration details
        config = status['configuration']
        click.echo(f"‚öôÔ∏è  Configuration:")
        click.echo(f"  Root: {config['root_directory']}")
        click.echo(f"  Auto-reload: {'enabled' if config['auto_reload_enabled'] else 'disabled'}")
        if config['last_reload']:
            click.echo(f"  Last reload: {config['last_reload']}")
        click.echo(f"  Tracked files: {config['tracked_files']}")
        click.echo("")
        
        # Validation summary
        validation = status['validation_summary']
        click.echo(f"üîç Validation Summary:")
        click.echo(f"  Total pipelines: {validation['total_pipelines']}")
        click.echo(f"  Valid: {validation['valid_pipelines']}")
        click.echo(f"  Invalid: {validation['invalid_pipelines']}")
        click.echo(f"  Total tables: {validation['total_tables']}")
        
        if validation['validation_errors']:
            click.echo("")
            click.echo("‚ùå Validation Errors:")
            for error in validation['validation_errors'][:5]:
                click.echo(f"  ‚Ä¢ {error}")
            if len(validation['validation_errors']) > 5:
                click.echo(f"  ... and {len(validation['validation_errors']) - 5} more")
    
    # Connection management commands
    @cli.group(name="connections")
    def connections_command():
        """Connection management for multi-schema support"""
        pass
    
    @connections_command.command(name="list")
    @click.option('--type', 'connection_type', type=click.Choice(['mysql', 'redshift', 'all']), 
                  default='all', help='Filter by connection type')
    def connections_list(connection_type: str):
        """List available database connections"""
        
        multi_schema_ctx.ensure_initialized()
        
        all_connections = multi_schema_ctx.connection_registry.list_connections()
        
        if not all_connections:
            click.echo("üîå No connections configured")
            click.echo("üí° Run 'python -m src.cli.main config setup' to create default configuration")
            return
        
        # Filter by type
        if connection_type != 'all':
            all_connections = {
                k: v for k, v in all_connections.items() 
                if v['type'] == connection_type
            }
        
        if not all_connections:
            click.echo(f"üîå No {connection_type} connections found")
            return
        
        click.echo("üîå Available Database Connections:")
        click.echo("")
        
        # Group by type
        sources = {k: v for k, v in all_connections.items() if v['type'] == 'mysql'}
        targets = {k: v for k, v in all_connections.items() if v['type'] == 'redshift'}
        
        if sources and connection_type in ['mysql', 'all']:
            click.echo("üóÑÔ∏è  MySQL Sources:")
            for name, info in sources.items():
                status_icon = "üü¢" if info['status'] == 'active' else "‚ö™"
                click.echo(f"  {status_icon} {name}")
                click.echo(f"      Host: {info['host']}:{info['port']}")
                click.echo(f"      Database: {info['database']}")
                click.echo(f"      User: {info['username']}")
                if info['description']:
                    click.echo(f"      Description: {info['description']}")
                click.echo("")
        
        if targets and connection_type in ['redshift', 'all']:
            click.echo("üéØ Redshift Targets:")
            for name, info in targets.items():
                status_icon = "üü¢" if info['status'] == 'active' else "‚ö™"
                tunnel_icon = "üîí" if info['has_ssh_tunnel'] else "üåê"
                click.echo(f"  {status_icon} {name} {tunnel_icon}")
                click.echo(f"      Host: {info['host']}:{info['port']}")
                click.echo(f"      Database: {info['database']} (schema: {info['schema']})")
                click.echo(f"      User: {info['username']}")
                if info['description']:
                    click.echo(f"      Description: {info['description']}")
                click.echo("")
    
    @connections_command.command(name="test")
    @click.argument('connection_name', required=False)
    @click.option('--all', 'test_all', is_flag=True, help='Test all connections')
    def connections_test(connection_name: Optional[str], test_all: bool):
        """Test database connections"""
        
        multi_schema_ctx.ensure_initialized()
        
        if test_all:
            click.echo("üîç Testing all connections...")
            click.echo("")
            
            results = multi_schema_ctx.connection_registry.test_all_connections()
            
            for conn_name, result in results['results'].items():
                if result['success']:
                    click.echo(f"‚úÖ {conn_name} ({result['duration_seconds']}s)")
                    if result['details']:
                        for key, value in result['details'].items():
                            if 'version' in key.lower():
                                click.echo(f"   {value}")
                else:
                    click.echo(f"‚ùå {conn_name}: {result['error']}")
                click.echo("")
            
            # Summary
            summary = results['summary']
            click.echo(f"üìä Summary: {summary['successful_connections']}/{summary['total_connections']} successful ({summary['success_rate']}%)")
            
            if summary['successful_connections'] < summary['total_connections']:
                sys.exit(1)
        
        elif connection_name:
            click.echo(f"üîç Testing connection: {connection_name}")
            
            result = multi_schema_ctx.connection_registry.test_connection(connection_name)
            
            if result['success']:
                click.echo(f"‚úÖ Connection successful ({result['duration_seconds']}s)")
                
                if result['details']:
                    click.echo("")
                    click.echo("Connection Details:")
                    for key, value in result['details'].items():
                        click.echo(f"  {key}: {value}")
            else:
                click.echo(f"‚ùå Connection failed: {result['error']}")
                sys.exit(1)
        
        else:
            click.echo("‚ùå Please specify a connection name or use --all")
            click.echo("üí° List connections: python -m src.cli.main connections list")
            sys.exit(1)
    
    @connections_command.command(name="info")
    @click.argument('connection_name')
    def connections_info(connection_name: str):
        """Show detailed connection information"""
        
        multi_schema_ctx.ensure_initialized()
        
        try:
            info = multi_schema_ctx.connection_registry.get_connection_info(connection_name)
            
            click.echo(f"üîå Connection: {info['name']}")
            click.echo("=" * 40)
            click.echo(f"Type: {info['type'].upper()}")
            click.echo(f"Host: {info['host']}:{info['port']}")
            click.echo(f"Database: {info['database']}")
            if info['schema']:
                click.echo(f"Schema: {info['schema']}")
            click.echo(f"Username: {info['username']}")
            click.echo(f"Status: {info['status']}")
            
            if info['description']:
                click.echo(f"Description: {info['description']}")
            
            if info['has_ssh_tunnel']:
                click.echo("üîí SSH Tunnel: Enabled")
            
            if info['pool_settings']:
                click.echo("")
                click.echo("üèä Connection Pool Settings:")
                for key, value in info['pool_settings'].items():
                    click.echo(f"  {key}: {value}")
        
        except Exception as e:
            click.echo(f"‚ùå Error getting connection info: {e}")
            sys.exit(1)
    
    @connections_command.command(name="health")
    def connections_health():
        """Show connection registry health status"""
        
        multi_schema_ctx.ensure_initialized()
        
        health = multi_schema_ctx.connection_registry.get_health_status()
        
        click.echo("üè• Connection Registry Health")
        click.echo("=" * 35)
        click.echo(f"Total connections: {health['total_connections']}")
        click.echo(f"Active MySQL pools: {health['mysql_pools_active']}")
        click.echo(f"Active SSH tunnels: {health['ssh_tunnels_active']}")
        click.echo("")
        
        click.echo("Connection Types:")
        for conn_type, count in health['connection_types'].items():
            click.echo(f"  {conn_type}: {count}")
        click.echo("")
        
        click.echo(f"Configuration file: {health['configuration_file']}")
        click.echo(f"Configuration exists: {'‚úÖ' if health['configuration_exists'] else '‚ùå'}")
        click.echo("")
        
        click.echo("Settings:")
        for key, value in health['settings'].items():
            click.echo(f"  {key}: {value}")


def _show_sync_help():
    """Show help for sync command options"""
    click.echo("üìö Multi-Schema Sync Options (v1.1.0):")
    click.echo("")
    click.echo("Pipeline-based sync (recommended):")
    click.echo("  python -m src.cli.main sync pipeline --pipeline PIPELINE_NAME --table TABLE1 TABLE2")
    click.echo("")
    click.echo("Connection-based sync:")
    click.echo("  python -m src.cli.main sync connections --source SOURCE_CONN --target TARGET_CONN --table TABLE1")
    click.echo("")
    click.echo("Legacy v1.0.0 syntax (still supported):")
    click.echo("  python -m src.cli.main sync --table settlement.table_name")
    click.echo("")
    click.echo("üí° Tips:")
    click.echo("  ‚Ä¢ List pipelines: python -m src.cli.main config list-pipelines")
    click.echo("  ‚Ä¢ List connections: python -m src.cli.main connections list")
    click.echo("  ‚Ä¢ Setup configuration: python -m src.cli.main config setup")


def _sync_legacy_mode(tables: List[str], backup_only: bool, redshift_only: bool, limit: Optional[int], max_workers: Optional[int], max_chunks: Optional[int]):
    """Handle v1.0.0 legacy sync mode"""
    click.echo("üîÑ v1.0.0 Compatibility Mode")
    
    # Use the existing v1.0.0 sync logic with proper integration
    try:
        # Import the backup strategies and connection manager
        from src.backup.sequential import SequentialBackupStrategy
        from src.core.connections import ConnectionManager
        from src.config.settings import AppConfig
        
        # Use v1.0.0 environment-based connection
        config = AppConfig()
        connection_manager = ConnectionManager(config)
        
        # Execute backup for each table using proper connection context
        success_count = 0
        for table_name in tables:
            try:
                click.echo(f"üìã Processing table: {table_name}")
                
                # Create v1.0.0 style backup strategy with connection manager
                backup_strategy = SequentialBackupStrategy(config)
                
                # TODO: Apply max_workers and max_chunks configuration to backup strategy
                # if max_workers:
                #     backup_strategy.set_max_workers(max_workers)
                # if max_chunks:
                #     backup_strategy.set_max_chunks(max_chunks)
                
                # Execute backup with the same logic as v1.0.0
                # SequentialBackupStrategy handles its own connection management
                tables_list = [table_name]
                result = backup_strategy.execute(
                    tables_list, 
                    chunk_size=limit if limit else config.backup.target_rows_per_chunk,
                    max_total_rows=limit if limit else None
                )
                
                if result:
                    click.echo(f"  ‚úÖ {table_name} completed successfully")
                    success_count += 1
                else:
                    click.echo(f"  ‚ùå {table_name} failed")
                    
            except Exception as e:
                click.echo(f"  ‚ùå {table_name} failed: {e}")
                logger.error(f"Legacy sync failed for {table_name}: {e}")
        
        # Summary
        total_tables = len(tables)
        if success_count == total_tables:
            click.echo(f"‚úÖ Legacy sync completed successfully: {success_count}/{total_tables} tables")
        else:
            click.echo(f"‚ö†Ô∏è  Legacy sync completed with issues: {success_count}/{total_tables} tables successful")
            
    except ImportError as e:
        click.echo(f"‚ùå Legacy sync components not available: {e}")
        click.echo("üí° Consider upgrading to pipeline-based syntax")
        return _fallback_to_legacy_sync_simple(tables, backup_only, redshift_only, limit)
    except Exception as e:
        click.echo(f"‚ùå Legacy sync failed: {e}")
        logger.error(f"Legacy sync mode failed: {e}")


def _fallback_to_legacy_sync(table_name: str, backup_only: bool, redshift_only: bool, limit: Optional[int]) -> bool:
    """Fallback sync function for single table when multi-schema fails"""
    try:
        from src.backup.sequential import SequentialBackupStrategy
        from src.config.settings import AppConfig
        
        # Use v1.0.0 backup strategy with proper configuration
        config = AppConfig()
        backup_strategy = SequentialBackupStrategy(config)
        
        # Execute backup for single table
        tables_list = [table_name]
        result = backup_strategy.execute(
            tables_list, 
            chunk_size=limit if limit else config.backup.target_rows_per_chunk,
            max_total_rows=limit if limit else None
        )
        
        return result
        
    except Exception as e:
        logger.error(f"Fallback legacy sync failed for {table_name}: {e}")
        return False


def _fallback_to_legacy_sync_simple(tables: List[str], backup_only: bool, redshift_only: bool, limit: Optional[int]):
    """Simple fallback when all else fails"""
    click.echo("‚ö†Ô∏è  Using basic fallback mode")
    
    for table_name in tables:
        success = _fallback_to_legacy_sync(table_name, backup_only, redshift_only, limit)
        if success:
            click.echo(f"  ‚úÖ {table_name} completed")
        else:
            click.echo(f"  ‚ùå {table_name} failed")


def _sync_with_default_pipeline(tables: List[str], backup_only: bool, redshift_only: bool, limit: Optional[int], max_workers: Optional[int], max_chunks: Optional[int]):
    """Handle sync with default pipeline configuration"""
    click.echo("üìã Using default pipeline configuration")
    
    # Get default pipeline and execute with new multi-schema system
    pipeline_config = multi_schema_ctx.config_manager.get_pipeline_config("default")
    
    # Register tables dynamically if needed
    for table_name in tables:
        if table_name not in pipeline_config.tables:
            multi_schema_ctx.config_manager.register_table_dynamically("default", table_name)
    
    # Execute sync using pipeline system
    success_count = 0
    for table_name in tables:
        table_config = pipeline_config.tables[table_name]
        success = _execute_table_sync(
            pipeline_config, table_config,
            backup_only, redshift_only, limit, False, max_workers, max_chunks
        )
        if success:
            success_count += 1
    
    if success_count == len(tables):
        click.echo(f"‚úÖ Sync completed: {success_count}/{len(tables)} tables")
    else:
        click.echo(f"‚ö†Ô∏è  Sync completed with issues: {success_count}/{len(tables)} tables")


def _preview_table_sync(pipeline_config, table_config, backup_only: bool, redshift_only: bool):
    """Preview table sync operations without execution"""
    click.echo(f"üîç Preview: {table_config.full_name}")
    click.echo(f"  Strategy: {table_config.cdc_strategy}")
    click.echo(f"  Type: {table_config.table_type}")
    click.echo(f"  Batch size: {table_config.processing.get('batch_size', 10000)}")
    
    if not redshift_only:
        click.echo(f"  üì§ MySQL extraction: {pipeline_config.source}")
        click.echo(f"    Source table: {table_config.full_name}")
        click.echo(f"    Timestamp column: {table_config.cdc_timestamp_column}")
        if table_config.cdc_strategy in ['hybrid', 'id_only']:
            click.echo(f"    ID column: {table_config.cdc_id_column}")
    
    if not backup_only:
        click.echo(f"  üì• Redshift loading: {pipeline_config.target}")
        # Show if it's a custom mapping or default
        if table_config.target_name != table_config.full_name:
            # Extract the default table name for comparison
            default_name = table_config.full_name
            if '.' in default_name:
                _, default_name = default_name.rsplit('.', 1)
            if table_config.target_name != default_name:
                click.echo(f"    Target table: {table_config.target_name} (custom mapping)")
            else:
                click.echo(f"    Target table: {table_config.target_name}")
        else:
            click.echo(f"    Target table: {table_config.target_name}")
    
    if table_config.depends_on:
        click.echo(f"  üìã Dependencies: {', '.join(table_config.depends_on)}")


def _execute_table_sync(pipeline_config, table_config, backup_only: bool, redshift_only: bool, 
                       limit: Optional[int], parallel: bool, max_workers: Optional[int] = None, max_chunks: Optional[int] = None) -> Dict[str, Any]:
    """Execute actual table sync with multi-schema configuration"""
    
    click.echo(f"üöÄ Syncing: {table_config.full_name}")
    
    try:
        # Import the existing backup strategies
        from src.backup.sequential import SequentialBackupStrategy
        from src.backup.inter_table import InterTableBackupStrategy
        
        # Use v1.0.0 backup strategy with proper configuration
        # The existing backup strategies are designed to work with AppConfig, not direct connections
        try:
            from src.config.settings import AppConfig
            
            # Create a temporary config that uses the same connection settings
            config = AppConfig()
            
            # Determine backup strategy based on pipeline configuration  
            strategy_name = pipeline_config.processing.get('strategy', 'sequential')
            if parallel:
                strategy_name = 'parallel'
            
            # Create backup strategy using the existing v1.0.0 pattern
            if strategy_name == 'parallel':
                backup_strategy = InterTableBackupStrategy(config)
            else:
                backup_strategy = SequentialBackupStrategy(config)
            
            # Pass pipeline configuration to backup strategy for CDC integration
            backup_strategy.pipeline_config = {
                'tables': {name: asdict(cfg) for name, cfg in pipeline_config.tables.items()}
            }
            
            # Configure strategy with pipeline settings
            if hasattr(backup_strategy, 'set_batch_size'):
                batch_size = table_config.processing.get('batch_size', 10000)
                backup_strategy.set_batch_size(batch_size)
            
            # Execute the sync using the table name
            # The backup strategy will handle its own connection management
            
            # FIXED: Generate scoped table name for v1.2.0 multi-schema S3 support
            # For connection-based sync, we need to scope the table name with connection identifier
            base_table_name = table_config.full_name
            
            # Check if we're in ad-hoc connection sync mode (pipeline name contains "adhoc_")
            if pipeline_config.name.startswith("adhoc_") and "_to_" in pipeline_config.name:
                # Extract source connection from ad-hoc pipeline name: "adhoc_US_DW_RO_SSH_to_redshift_default"
                parts = pipeline_config.name.split("_to_")
                if len(parts) >= 2:
                    source_connection = parts[0].replace("adhoc_", "")
                    scoped_table_name = f"{source_connection}:{base_table_name}"
                    logger.info(f"Using scoped table name for v1.2.0 S3 isolation: {scoped_table_name}")
                else:
                    scoped_table_name = base_table_name
            else:
                # Regular pipeline sync - use scoped table name for v1.2.0 multi-schema consistency
                source_connection = pipeline_config.source
                scoped_table_name = f"{source_connection}:{base_table_name}"
                logger.info(f"Using scoped table name for v1.2.0 pipeline: {scoped_table_name}")
            
            table_list = [scoped_table_name]
            
            # Calculate parameters for the backup
            chunk_size = limit if limit else table_config.processing.get('batch_size', 10000)
            max_total_rows = None
            
            if limit and max_chunks:
                # limit = chunk size, max_chunks = number of chunks
                # total rows = limit √ó max_chunks
                max_total_rows = limit * max_chunks
                logger.info(f"Row limits for {base_table_name}: {chunk_size} rows/chunk √ó {max_chunks} chunks = {max_total_rows} total rows")
            elif limit and not max_chunks:
                # limit = total row limit (user expectation for --limit 100)
                max_total_rows = limit
                chunk_size = min(limit, table_config.processing.get('batch_size', 10000))
                logger.info(f"Total row limit for {base_table_name}: {max_total_rows} rows (chunk size: {chunk_size})")
            elif max_chunks and not limit:
                # max_chunks specified but no limit - use default chunk size
                max_total_rows = chunk_size * max_chunks
                logger.info(f"Row limits for {base_table_name}: {chunk_size} rows/chunk √ó {max_chunks} chunks = {max_total_rows} total rows")
            
            # FIXED: Properly handle backup_only and redshift_only flags (v1.2.0 regression fix)
            backup_success = True
            redshift_success = True
            
            if not redshift_only:
                # Stage 1: MySQL ‚Üí S3 Backup with multi-schema support
                source_connection = pipeline_config.source
                backup_success = backup_strategy.execute(
                    tables=table_list,
                    chunk_size=chunk_size,
                    max_total_rows=max_total_rows,
                    source_connection=source_connection
                )
            
            if not backup_only and backup_success:
                # Stage 2: S3 ‚Üí Redshift Loading (using v1.0.0 working pattern)
                try:
                    from src.core.gemini_redshift_loader import GeminiRedshiftLoader
                    from src.core.watermark_adapter import create_watermark_manager
                    from src.core.cdc_configuration_manager import CDCConfigurationManager
                    
                    # Use the watermark v2.0 system for Redshift loading with shared connection registry
                    # Pass the connection registry from backup to ensure SSH tunnels are reused
                    connection_registry = getattr(backup_strategy, 'connection_registry', None)
                    redshift_loader = GeminiRedshiftLoader(config, connection_registry=connection_registry)
                    watermark_manager = create_watermark_manager(config.to_dict())
                    
                    # Create CDC strategy for full_sync replace mode support
                    cdc_strategy = None
                    try:
                        cdc_config_manager = CDCConfigurationManager()
                        # Convert table_config to dict for CDC parsing
                        # Get batch_size with proper fallback hierarchy
                        # 1. Table-specific processing.batch_size (highest priority)
                        batch_size = table_config.processing.get('batch_size')
                        if batch_size is None:
                            # 2. Pipeline processing.batch_size (fallback)
                            pipeline_processing = config.pipeline.processing if hasattr(config.pipeline, 'processing') else {}
                            batch_size = pipeline_processing.get('batch_size')
                        if batch_size is None:
                            # 3. System default from AppConfig (final fallback)
                            batch_size = config.backup.target_rows_per_chunk
                        
                        table_config_dict = {
                            'cdc_strategy': table_config.cdc_strategy,
                            'cdc_timestamp_column': table_config.cdc_timestamp_column,
                            'cdc_id_column': table_config.cdc_id_column,
                            'full_sync_mode': table_config.full_sync_mode,
                            'batch_size': batch_size
                        }
                        cdc_strategy = cdc_config_manager.create_cdc_strategy(table_config_dict, base_table_name)
                        if cdc_strategy:
                            strategy_mode = cdc_strategy.get_sync_mode() if hasattr(cdc_strategy, 'get_sync_mode') else 'unknown'
                            logger.info(f"Created CDC strategy for {base_table_name}: {cdc_strategy.strategy_name} ({strategy_mode} mode)")
                    except Exception as cdc_e:
                        logger.warning(f"Failed to create CDC strategy for {base_table_name}: {cdc_e}")
                        # Continue without CDC strategy - will use default behavior
                    
                    # Test Redshift connection first
                    connection_test = redshift_loader._test_connection()
                    if not connection_test:
                        raise Exception("Redshift connection test failed")
                    
                    # Load table data to Redshift using the scoped name for S3 file discovery
                    # Pass CDC strategy for full_sync replace mode TRUNCATE support and table_config for target name mapping
                    redshift_success = redshift_loader.load_table_data(scoped_table_name, cdc_strategy, table_config)
                    
                except Exception as redshift_error:
                    logger.error(f"Redshift loading failed for {scoped_table_name}: {redshift_error}")
                    redshift_success = False
            
            # Overall result
            result = backup_success and redshift_success
            
            # FIXED: Get actual metrics from watermark system instead of hardcoded values
            actual_rows = 0
            actual_files = 0
            
            if result:
                try:
                    # Get actual metrics from watermark
                    from src.core.watermark_adapter import create_watermark_manager
                    watermark_manager = create_watermark_manager(config.to_dict())
                    watermark = watermark_manager.get_table_watermark(scoped_table_name)
                    
                    if watermark:
                        # Get actual rows processed from watermark
                        backup_rows = getattr(watermark, 'mysql_rows_extracted', 0) or 0
                        redshift_rows = getattr(watermark, 'redshift_rows_loaded', 0) or 0
                        
                        # Use the most relevant metric based on operation type
                        if redshift_only:
                            actual_rows = redshift_rows
                        elif backup_only:
                            actual_rows = backup_rows
                        else:
                            # Full sync - use redshift rows (final loaded count)
                            actual_rows = redshift_rows
                        
                        # Get file count from processed S3 files list
                        processed_files = getattr(watermark, 'processed_s3_files', []) or []
                        actual_files = len(processed_files)
                        
                        logger.info(f"METRICS: {scoped_table_name} - {actual_rows} rows, {actual_files} files")
                    
                except Exception as metrics_error:
                    logger.warning(f"Failed to get actual metrics for {scoped_table_name}: {metrics_error}")
                    # Fall back to estimated values based on chunk processing
                    actual_rows = max_total_rows if max_total_rows else 0
                    actual_files = 1
            
            result_dict = {
                "success": result,
                "rows_processed": actual_rows,
                "files_created": actual_files,
                "duration": 30.0  # TODO: Add actual duration tracking
            }
            
            if result:
                click.echo(f"  ‚úÖ {table_config.full_name} synced successfully")
            else:
                click.echo(f"  ‚ùå {table_config.full_name} sync failed")
                
            return result_dict
            
        except Exception as config_error:
            logger.error(f"Failed to execute table sync with v1.0.0 compatibility: {config_error}")
            click.echo(f"  ‚ùå Configuration error: {config_error}")
            return {
                "success": False,
                "error_message": str(config_error),
                "rows_processed": 0,
                "files_created": 0,
                "duration": 0
            }
            
    except ImportError as e:
        # Fall back to v1.0.0 style sync
        logger.warning(f"Multi-schema backup strategies not available, falling back to v1.0.0 mode: {e}")
        legacy_result = _fallback_to_legacy_sync(table_config.full_name, backup_only, redshift_only, limit)
        return {
            "success": legacy_result,
            "rows_processed": 0,  # Legacy mode doesn't provide metrics
            "files_created": 0,
            "duration": 0
        }
        
    except Exception as e:
        click.echo(f"  ‚ùå {table_config.full_name} failed: {e}")
        logger.error(f"Table sync failed for {table_config.full_name}: {e}")
        return {
            "success": False,
            "error_message": str(e),
            "rows_processed": 0,
            "files_created": 0,
            "duration": 0
        }


def _create_adhoc_pipeline_config(source: str, target: str, tables: List[str], batch_size: int):
    """Create ad-hoc pipeline configuration for connection-based sync"""
    from src.core.configuration_manager import PipelineConfig, TableConfig
    
    # Create table configurations with proper timestamp column detection
    table_configs = {}
    for table_name in tables:
        # FIXED: Look up table-specific configuration from existing pipelines
        timestamp_column = "updated_at"  # Default
        id_column = "id"  # Default
        
        # Try to find existing configuration for this table in any pipeline
        if multi_schema_ctx.config_manager:
            try:
                # Check us_dw_pipeline first (maps to US_DW_RO_SSH connection)
                if source == "US_DW_RO_SSH":
                    pipeline_config = multi_schema_ctx.config_manager.get_pipeline_config("us_dw_pipeline")
                    if pipeline_config and table_name in pipeline_config.tables:
                        table_def = pipeline_config.tables[table_name]
                        timestamp_column = table_def.cdc_timestamp_column or timestamp_column
                        id_column = table_def.cdc_id_column or id_column
                        logger.info(f"Found pipeline config for {table_name}: timestamp={timestamp_column}, id={id_column}")
                    else:
                        logger.info(f"No specific config found for {table_name}, using defaults")
                else:
                    # For other connections, try to find matching pipeline
                    for pipeline_name in multi_schema_ctx.config_manager.list_pipelines():
                        try:
                            pipeline_config = multi_schema_ctx.config_manager.get_pipeline_config(pipeline_name)
                            if pipeline_config and pipeline_config.source == source and table_name in pipeline_config.tables:
                                table_def = pipeline_config.tables[table_name]
                                timestamp_column = table_def.cdc_timestamp_column or timestamp_column
                                id_column = table_def.cdc_id_column or id_column
                                logger.info(f"Found pipeline config in {pipeline_name} for {table_name}: timestamp={timestamp_column}, id={id_column}")
                                break
                        except:
                            continue
            except Exception as e:
                logger.warning(f"Failed to lookup pipeline config for {table_name}: {e}")
        
        table_configs[table_name] = TableConfig(
            full_name=table_name,
            cdc_strategy="hybrid",
            cdc_timestamp_column=timestamp_column,
            cdc_id_column=id_column,
            processing={'batch_size': batch_size}
        )
    
    return PipelineConfig(
        name=f"adhoc_{source}_to_{target}",
        version="1.1.0",
        description=f"Ad-hoc pipeline: {source} ‚Üí {target}",
        source=source,
        target=target,
        processing={'strategy': 'sequential', 'batch_size': batch_size},
        s3={'isolation_prefix': f"adhoc_{source}_{target}", 'partition_strategy': 'hybrid'},
        default_table_config={},
        tables=table_configs
    )