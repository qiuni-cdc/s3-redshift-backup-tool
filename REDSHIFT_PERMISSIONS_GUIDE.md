# Redshift æƒé™å’Œè°ƒè¯•æŒ‡å—

## ğŸ” Redshift ç³»ç»Ÿè¡¨æƒé™è¯´æ˜

### **ç³»ç»Ÿè¡¨è®¿é—®æƒé™è¦æ±‚**

Redshift çš„ç³»ç»Ÿè¡¨ï¼ˆ`STV_*`, `STL_*`, `SVL_*`, `SVV_*`ï¼‰éœ€è¦**ä¸åŒçº§åˆ«çš„æƒé™**ï¼š

#### **éœ€è¦è¶…çº§ç”¨æˆ·æƒé™çš„ç³»ç»Ÿè¡¨** âŒ
è¿™äº›è¡¨éœ€è¦ `SUPERUSER` æƒé™æˆ–ç‰¹å®šçš„ç³»ç»Ÿæƒé™æ‰èƒ½è®¿é—®ï¼š

```sql
-- éœ€è¦è¶…çº§ç”¨æˆ·æƒé™
STV_RECENTS          -- å½“å‰è¿è¡Œçš„æŸ¥è¯¢
STV_INFLIGHT         -- æ­£åœ¨æ‰§è¡Œçš„æŸ¥è¯¢è¯¦æƒ…
STV_LOCKS           -- é”ä¿¡æ¯
STV_SESSIONS        -- ä¼šè¯ä¿¡æ¯
STV_WLM_SERVICE_CLASS_STATE  -- WLMé˜Ÿåˆ—çŠ¶æ€
```

**é”™è¯¯ç¤ºä¾‹**ï¼š
```
ERROR: permission denied for relation stv_recents
```

#### **æ™®é€šç”¨æˆ·å¯è®¿é—®çš„ç³»ç»Ÿè§†å›¾** âœ…
è¿™äº›è§†å›¾æ™®é€šç”¨æˆ·ä¹Ÿå¯ä»¥è®¿é—®ï¼š

```sql
-- æ™®é€šç”¨æˆ·å¯è®¿é—®
PG_TABLES           -- è¡¨ä¿¡æ¯
PG_LOCKS            -- è‡ªå·±ä¼šè¯çš„é”ä¿¡æ¯ï¼ˆéƒ¨åˆ†ï¼‰
PG_STAT_ACTIVITY    -- è‡ªå·±çš„ä¼šè¯æ´»åŠ¨ï¼ˆéƒ¨åˆ†ï¼‰
INFORMATION_SCHEMA  -- æ ‡å‡† SQL ä¿¡æ¯æ¨¡å¼è§†å›¾
STL_LOAD_ERRORS     -- COPYé”™è¯¯ï¼ˆè‡ªå·±ç”¨æˆ·çš„ï¼‰
```

---

## ğŸ› ï¸ **æ— éœ€ç³»ç»Ÿè¡¨æƒé™çš„è°ƒè¯•æ–¹æ¡ˆ**

### **æ–¹æ¡ˆ 1: ä½¿ç”¨åº”ç”¨å±‚è¯Šæ–­å·¥å…·** â­ **æ¨è**

æˆ‘å·²ç»ä¸ºä½ åˆ›å»ºäº†è¯Šæ–­å·¥å…·ï¼Œè¿è¡Œæ–¹æ³•ï¼š

```bash
# åŸºç¡€è¯Šæ–­ï¼ˆæµ‹è¯•è¿æ¥ã€å»¶è¿Ÿã€é…ç½®ï¼‰
python scripts/debug_redshift_copy.py \
  --config config/connections.yml \
  --env us_dw

# å¸¦ S3 COPY æµ‹è¯•ï¼ˆæ£€æµ‹æ˜¯å¦ä¼šå¡ä½ï¼‰
python scripts/debug_redshift_copy.py \
  --config config/connections.yml \
  --env us_dw \
  --test-s3 "s3://your-bucket/incremental/test.parquet"
```

**è¿™ä¸ªå·¥å…·ä¼šæ£€æµ‹**ï¼š
- âœ… Redshift è¿æ¥æ˜¯å¦æ­£å¸¸
- âœ… ç½‘ç»œå»¶è¿Ÿæ˜¯å¦è¿‡é«˜
- âœ… SSH éš§é“æ˜¯å¦ç¨³å®š
- âœ… COPY æ“ä½œæ˜¯å¦è¶…æ—¶/å¡ä½
- âœ… è¡¨æ˜¯å¦å­˜åœ¨å’Œæ•°æ®æ˜¯å¦åŠ è½½æˆåŠŸ

### **æ–¹æ¡ˆ 2: ä½¿ç”¨ pg_catalog è§†å›¾**

è¿™äº›è§†å›¾ä¸éœ€è¦ç‰¹æ®Šæƒé™ï¼š

```sql
-- 1. æŸ¥çœ‹å½“å‰ä¼šè¯ä¿¡æ¯
SELECT
    current_database() as database,
    current_user as user,
    pg_backend_pid() as my_pid,
    now() as current_time,
    version() as version;

-- 2. æŸ¥çœ‹è¡¨ä¿¡æ¯
SELECT
    schemaname,
    tablename,
    tableowner,
    hasindexes,
    hasrules
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY tablename;

-- 3. éªŒè¯æ•°æ®æ˜¯å¦åŠ è½½æˆåŠŸ
SELECT COUNT(*) FROM public.your_table_name;

SELECT * FROM public.your_table_name LIMIT 5;

-- 4. æŸ¥çœ‹è‡ªå·±çš„é”
SELECT
    locktype,
    relation::regclass as table_name,
    mode,
    granted
FROM pg_locks
WHERE pid = pg_backend_pid();

-- 5. æŸ¥çœ‹ COPY é”™è¯¯ï¼ˆåªèƒ½çœ‹åˆ°è‡ªå·±ç”¨æˆ·çš„ï¼‰
SELECT
    query,
    filename,
    line_number,
    colname,
    err_code,
    err_reason
FROM stl_load_errors
WHERE starttime > CURRENT_TIMESTAMP - INTERVAL '1 hour'
  AND userid = (SELECT usesysid FROM pg_user WHERE usename = current_user)
ORDER BY starttime DESC
LIMIT 20;
```

### **æ–¹æ¡ˆ 3: ä½¿ç”¨è¶…æ—¶æœºåˆ¶æ£€æµ‹å¡ä½**

åœ¨ä»£ç ä¸­æ·»åŠ è¶…æ—¶æ£€æµ‹ï¼ˆå·²åœ¨è¯Šæ–­å·¥å…·ä¸­å®ç°ï¼‰ï¼š

```python
from src.utils.redshift_diagnostics import query_timeout

# åœ¨ COPY å‘½ä»¤æ‰§è¡Œæ—¶æ·»åŠ è¶…æ—¶
with query_timeout(300):  # 5åˆ†é’Ÿè¶…æ—¶
    cursor.execute("COPY table FROM 's3://...' ...")

# å¦‚æœè¶…æ—¶ä¼šæŠ›å‡º TimeoutException
```

### **æ–¹æ¡ˆ 4: ä»åº”ç”¨æ—¥å¿—åˆ†æ**

æ£€æŸ¥åº”ç”¨æ—¥å¿—ä¸­çš„å…³é”®ä¿¡æ¯ï¼š

```bash
# æŸ¥æ‰¾ COPY ç›¸å…³æ—¥å¿—
grep -i "executing copy\|copy command" logs/*.log | tail -20

# æŸ¥æ‰¾è¶…æ—¶é”™è¯¯
grep -i "timeout\|timed out" logs/*.log

# æŸ¥æ‰¾ SSH éš§é“é—®é¢˜
grep -i "tunnel\|ssh" logs/*.log

# æŸ¥æ‰¾ S3 è®¿é—®é”™è¯¯
grep -i "s3.*error\|access denied\|403" logs/*.log
```

### **æ–¹æ¡ˆ 5: CloudWatch ç›‘æ§ï¼ˆAWS æ§åˆ¶å°ï¼‰**

å¦‚æœä½ æœ‰ AWS æ§åˆ¶å°è®¿é—®æƒé™ï¼Œå¯ä»¥åœ¨ Redshift æ§åˆ¶å°æŸ¥çœ‹ï¼š

1. **Redshift Console** â†’ **Queries and loads**
   - æŸ¥çœ‹è¿è¡Œä¸­çš„æŸ¥è¯¢
   - æŸ¥çœ‹ COPY æ“ä½œçŠ¶æ€
   - æŸ¥çœ‹é”™è¯¯ä¿¡æ¯

2. **CloudWatch Metrics**
   - `DatabaseConnections` - è¿æ¥æ•°
   - `PercentageDiskSpaceUsed` - ç£ç›˜ä½¿ç”¨ç‡
   - `CPUUtilization` - CPUä½¿ç”¨ç‡
   - `ReadLatency/WriteLatency` - I/Oå»¶è¿Ÿ

3. **Redshift Query Monitoring**
   - æŸ¥çœ‹æ…¢æŸ¥è¯¢
   - æŸ¥çœ‹é˜Ÿåˆ—ç­‰å¾…æ—¶é—´
   - æŸ¥çœ‹WLMé…ç½®

---

## ğŸ“‹ **å¦‚ä½•è¯·æ±‚æœ€å°å¿…è¦æƒé™**

å¦‚æœéœ€è¦å‘DBAç”³è¯·æƒé™ï¼Œå¯ä»¥è¯·æ±‚ä»¥ä¸‹**æœ€å°æƒé™é›†**ï¼š

### **é€‰é¡¹ 1: åªè¯»ç›‘æ§è§†å›¾è®¿é—®** ï¼ˆæœ€å°æƒé™ï¼‰

```sql
-- æˆäºˆå¯¹ç‰¹å®šç³»ç»Ÿè§†å›¾çš„ SELECT æƒé™
GRANT SELECT ON stl_load_errors TO your_user;
GRANT SELECT ON svl_query_summary TO your_user;
GRANT SELECT ON svv_table_info TO your_user;
```

### **é€‰é¡¹ 2: ç›‘æ§è§’è‰²** ï¼ˆæ¨èï¼‰

```sql
-- åˆ›å»ºç›‘æ§è§’è‰²å¹¶æˆäºˆæƒé™
CREATE ROLE monitoring_role;

-- æˆäºˆç³»ç»Ÿè§†å›¾è®¿é—®
GRANT SELECT ON stl_load_errors TO monitoring_role;
GRANT SELECT ON stl_query TO monitoring_role;
GRANT SELECT ON svl_statementtext TO monitoring_role;
GRANT SELECT ON svv_table_info TO monitoring_role;

-- å°†è§’è‰²åˆ†é…ç»™ç”¨æˆ·
GRANT monitoring_role TO your_user;
```

### **é€‰é¡¹ 3: åˆ›å»ºè‡ªå®šä¹‰ç›‘æ§è§†å›¾**

è®© DBA åˆ›å»ºä¸€ä¸ªè§†å›¾ï¼Œæ™®é€šç”¨æˆ·å¯ä»¥è®¿é—®ï¼š

```sql
-- DBA åˆ›å»ºç›‘æ§è§†å›¾
CREATE VIEW public.copy_monitoring AS
SELECT
    query,
    starttime,
    duration/1000000 as duration_seconds,
    querytxt
FROM stl_query
WHERE querytxt ILIKE '%COPY%'
  AND starttime > CURRENT_DATE - 1;

-- æˆäºˆè®¿é—®æƒé™
GRANT SELECT ON public.copy_monitoring TO your_user;
```

---

## ğŸ” **å®ç”¨è°ƒè¯•å‘½ä»¤ï¼ˆæ— éœ€ç‰¹æ®Šæƒé™ï¼‰**

### **æ£€æŸ¥ COPY æ˜¯å¦æˆåŠŸ**

```sql
-- æŸ¥çœ‹æœ€è¿‘çš„ COPY å‘½ä»¤ï¼ˆä»è‡ªå·±çš„æŸ¥è¯¢å†å²ï¼‰
SELECT
    query,
    SUBSTRING(querytxt, 1, 100) as query_text,
    starttime,
    endtime,
    DATEDIFF(second, starttime, endtime) as duration_seconds
FROM stl_query
WHERE userid = (SELECT usesysid FROM pg_user WHERE usename = current_user)
  AND querytxt ILIKE '%COPY%'
ORDER BY starttime DESC
LIMIT 10;

-- æ³¨æ„ï¼šè¿™ä¸ªæŸ¥è¯¢åªèƒ½çœ‹åˆ°è‡ªå·±ç”¨æˆ·çš„å†å²
```

### **æ£€æŸ¥è¡¨æ•°æ®**

```sql
-- éªŒè¯è¡¨å­˜åœ¨
SELECT
    schemaname,
    tablename,
    tableowner
FROM pg_tables
WHERE tablename = 'your_table'
  AND schemaname = 'public';

-- æ£€æŸ¥è¡Œæ•°
SELECT COUNT(*) as total_rows
FROM public.your_table;

-- æŸ¥çœ‹æœ€è¿‘çš„æ•°æ®ï¼ˆå¦‚æœæœ‰æ—¶é—´æˆ³åˆ—ï¼‰
SELECT *
FROM public.your_table
ORDER BY created_at DESC
LIMIT 10;

-- æ£€æŸ¥è¡¨å¤§å°
SELECT
    table_schema,
    table_name,
    pg_size_pretty(pg_total_relation_size(quote_ident(table_schema)||'.'||quote_ident(table_name))) AS size
FROM information_schema.tables
WHERE table_name = 'your_table';
```

### **æµ‹è¯•è¿æ¥å’Œæ€§èƒ½**

```python
# ä½¿ç”¨è¯Šæ–­å·¥å…·
from src.utils.redshift_diagnostics import RedshiftDiagnostics
import psycopg2

conn = psycopg2.connect(...)
diagnostics = RedshiftDiagnostics(conn)

# æµ‹è¯•å»¶è¿Ÿ
result = diagnostics.test_network_latency()
print(f"å¹³å‡å»¶è¿Ÿ: {result['avg_latency_ms']} ms")

# æµ‹è¯• COPYï¼ˆå¸¦è¶…æ—¶ï¼‰
result = diagnostics.test_s3_copy_simple(
    s3_uri="s3://bucket/file.parquet",
    aws_access_key="...",
    aws_secret_key="...",
    timeout_seconds=60
)

if result['timed_out']:
    print("âŒ COPY æ“ä½œè¶…æ—¶ï¼Œå¯èƒ½å¡ä½äº†")
else:
    print(f"âœ… COPY æˆåŠŸ: {result['rows_loaded']} è¡Œ")
```

---

## ğŸš€ **å¿«é€Ÿæ•…éšœæ’æŸ¥æµç¨‹**

### **æ­¥éª¤ 1: è¿è¡Œè¯Šæ–­è„šæœ¬**

```bash
python scripts/debug_redshift_copy.py --env us_dw
```

### **æ­¥éª¤ 2: æ£€æŸ¥è¡¨æ•°æ®**

```sql
-- è¿æ¥åˆ° Redshift
psql -h localhost -p <tunnel_port> -U <user> -d <database>

-- æ£€æŸ¥è¡¨
SELECT COUNT(*) FROM public.target_table;
```

### **æ­¥éª¤ 3: æŸ¥çœ‹åº”ç”¨æ—¥å¿—**

```bash
# æŸ¥çœ‹æœ€è¿‘çš„ COPY æ“ä½œ
tail -100 logs/sync.log | grep -i "copy"

# æŸ¥çœ‹é”™è¯¯
tail -100 logs/sync.log | grep -i "error\|failed"
```

### **æ­¥éª¤ 4: æ£€æŸ¥ watermark**

```bash
# æŸ¥çœ‹ S3 ä¸Šçš„ watermark
aws s3 ls s3://your-bucket/watermark/

# ä¸‹è½½å¹¶æŸ¥çœ‹
aws s3 cp s3://your-bucket/watermark/table_name_watermark.json -
```

---

## â“ **å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**

### **Q1: COPY æ“ä½œå¡ä½ä¸åŠ¨æ€ä¹ˆåŠï¼Ÿ**

**A**: ä½¿ç”¨è¯Šæ–­å·¥å…·æ£€æµ‹ï¼š
```bash
python scripts/debug_redshift_copy.py --env us_dw --test-s3 "s3://bucket/file.parquet"
```

å¦‚æœè¶…æ—¶ï¼Œæ£€æŸ¥ï¼š
- SSH éš§é“æ˜¯å¦ç¨³å®š
- S3 æ–‡ä»¶æ˜¯å¦å¯è®¿é—®
- Redshift é›†ç¾¤æ˜¯å¦æš‚åœ

### **Q2: å¦‚ä½•çŸ¥é“ COPY æ˜¯å¦æˆåŠŸï¼Ÿ**

**A**: ä¸‰ç§æ–¹æ³•éªŒè¯ï¼š

1. **æŸ¥è¯¢è¡¨è¡Œæ•°**
   ```sql
   SELECT COUNT(*) FROM public.target_table;
   ```

2. **æŸ¥çœ‹åº”ç”¨æ—¥å¿—**
   ```bash
   grep "âœ… COPY command loaded" logs/sync.log
   ```

3. **æ£€æŸ¥ watermark**
   ```bash
   aws s3 cp s3://bucket/watermark/table_watermark.json -
   ```

### **Q3: çœ‹ä¸åˆ° stv_recents æ€ä¹ˆæŸ¥è¿è¡Œä¸­çš„æŸ¥è¯¢ï¼Ÿ**

**A**: ä½¿ç”¨ä»¥ä¸‹æ›¿ä»£æ–¹æ¡ˆï¼š

1. **åœ¨åº”ç”¨ä¸­æ·»åŠ è¶…æ—¶æ£€æµ‹**ï¼ˆå·²å®ç°åœ¨è¯Šæ–­å·¥å…·ä¸­ï¼‰
2. **æ£€æŸ¥åº”ç”¨æ—¥å¿—çš„æ—¶é—´æˆ³**åˆ¤æ–­æ˜¯å¦å¡ä½
3. **ä½¿ç”¨ CloudWatch** æŸ¥çœ‹ Redshift æŒ‡æ ‡
4. **è¯·æ±‚ DBA** å¸®å¿™æŸ¥çœ‹ç³»ç»Ÿè¡¨

### **Q4: æ²¡æœ‰è¶…çº§ç”¨æˆ·æƒé™ï¼Œå¦‚ä½•è°ƒè¯•ï¼Ÿ**

**A**: ä½¿ç”¨æˆ‘åˆ›å»ºçš„è¯Šæ–­å·¥å…·ï¼š
- `scripts/debug_redshift_copy.py` - è‡ªåŠ¨æ£€æµ‹é—®é¢˜
- `src/utils/redshift_diagnostics.py` - Python API

è¿™äº›å·¥å…·**ä¸éœ€è¦ç³»ç»Ÿè¡¨æƒé™**ï¼Œé€šè¿‡åº”ç”¨å±‚æ£€æµ‹é—®é¢˜ã€‚

---

## ğŸ“ **æ€»ç»“**

| è°ƒè¯•æ–¹æ³• | éœ€è¦æƒé™ | æ¨èåº¦ | é€‚ç”¨åœºæ™¯ |
|---------|---------|--------|---------|
| åº”ç”¨å±‚è¯Šæ–­å·¥å…· | âŒ ä¸éœ€è¦ | â­â­â­â­â­ | é¦–é€‰ï¼Œè‡ªåŠ¨æ£€æµ‹ |
| pg_catalog è§†å›¾ | âŒ ä¸éœ€è¦ | â­â­â­â­ | æŸ¥çœ‹è¡¨å’Œæ•°æ® |
| åº”ç”¨æ—¥å¿—åˆ†æ | âŒ ä¸éœ€è¦ | â­â­â­â­ | è¿½è¸ªæ‰§è¡Œæµç¨‹ |
| CloudWatch æ§åˆ¶å° | AWS æ§åˆ¶å° | â­â­â­ | å¯è§†åŒ–ç›‘æ§ |
| stl_load_errors | âŒ ä¸éœ€è¦* | â­â­â­ | æŸ¥çœ‹ COPY é”™è¯¯ |
| ç³»ç»Ÿè¡¨ (stv_*) | âœ… éœ€è¦è¶…çº§ç”¨æˆ· | â­â­ | æœ€è¯¦ç»†ä¿¡æ¯ |

*æ³¨ï¼šstl_load_errors åªèƒ½çœ‹åˆ°è‡ªå·±ç”¨æˆ·çš„é”™è¯¯

**æ¨èä½¿ç”¨é¡ºåº**ï¼š
1. è¿è¡Œ `debug_redshift_copy.py` è¯Šæ–­è„šæœ¬
2. æ£€æŸ¥åº”ç”¨æ—¥å¿—
3. æŸ¥è¯¢ `stl_load_errors` è¡¨
4. è¿æ¥ Redshift éªŒè¯è¡¨æ•°æ®
5. å¦‚æœä»æ— æ³•å®šä½ï¼Œè¯·æ±‚ DBA ååŠ©
