# docker-compose.yml - Airflow + dbt for Order Tracking Pipeline
# Versions: Airflow 2.8.1, dbt 1.8.0 (matches QA server)
version: '3.8'

x-airflow-common: &airflow-common
  image: apache/airflow:2.8.1
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__LOAD_EXAMPLES=false
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    - AIRFLOW__WEBSERVER__SECRET_KEY=my_secret_key_for_local_testing
    - _PIP_ADDITIONAL_REQUIREMENTS=dbt-redshift==1.8.0 mysql-connector-python boto3 pyarrow pyyaml python-dotenv apache-airflow-providers-mysql apache-airflow-providers-postgres pydantic-settings pydantic sshtunnel paramiko structlog click tqdm pandas psycopg2-binary cerberus jsonschema typing-extensions
    # Paths for DAG (inside container)
    - SYNC_TOOL_PATH=/opt/airflow/sync_tool
    - DBT_PROJECT_PATH=/opt/airflow/dbt_projects/order_tracking
    - DBT_VENV_PATH=/home/airflow/.local
    # Source Database (MySQL)
    -  DB_HOST=us-west-2.ro.db.uniuni.com.internal
    -  DB_USER=jasleentung
    -  DB_PASSWORD=86a05b5772c55c83c9db74a01c01441f
    -  DB_DATABASE=kuaisong
    # dbt Redshift credentials (replace with actual values)
    - REDSHIFT_USER=sett_ddl_owner
    - "REDSHIFT_PASSWORD=Qx2[,y4*voli3)M>"
    - REDSHIFT_QA_USER=sett_ddl_owner
    - "REDSHIFT_QA_PASSWORD=Qx2[,y4*voli3)M>"
    - DBT_REDSHIFT_DB=dw
    # SSH keys
    - SSH_BASTION_KEY_PATH=/keys/jasleentung.pem
    # Source (MySQL) - Connect via SSH Tunnel
    - SSH_BASTION_HOST=35.83.114.196
    - SSH_BASTION_USER=jasleentung
    # SSH tunnel for local Docker testing (set to false on server)
    - DBT_USE_SSH_TUNNEL=false
    - DBT_REDSHIFT_HOST=redshift-dw.uniuni.com
    - DBT_REDSHIFT_PORT=5439
    # SSH key path inside container (for tunnel) - must match path in .env file
    - REDSHIFT_SSH_KEY_PATH= ~/etl/etl_dw/ssh/jasleentung.pem:/keys/jasleentung.pem:ro
  volumes:
    - ./dags:/opt/airflow/dags
    - ./dbt_projects:/opt/airflow/dbt_projects
    - ../src:/opt/airflow/src
    - ../config:/opt/airflow/config
    # Mount the actual SSH key FILE to the path expected by .env
    - ~/etl/etl_dw/ssh/jasleentung.pem:/keys/jasleentung.pem:ro
  depends_on:
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create --username admin --password admin123 --firstname Admin --lastname User --role Admin --email admin@example.com
    depends_on:
      postgres:
        condition: service_healthy

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8081:8080
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler

volumes:
  postgres-db-volume: