# docker-compose.yml - Airflow + dbt for Order Tracking Pipeline
# Versions: Airflow 2.8.1, dbt 1.8.0 (matches QA server)
version: '3.8'

x-airflow-common: &airflow-common
  image: apache/airflow:2.8.1
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__LOAD_EXAMPLES=false
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    - AIRFLOW__WEBSERVER__SECRET_KEY=my_secret_key_for_local_testing
    - _PIP_ADDITIONAL_REQUIREMENTS=dbt-redshift==1.8.0 mysql-connector-python boto3 pyarrow pyyaml python-dotenv apache-airflow-providers-mysql apache-airflow-providers-postgres pydantic-settings pydantic sshtunnel paramiko structlog click tqdm pandas psycopg2-binary cerberus jsonschema typing-extensions
    # Paths for DAG (inside container)
    - SYNC_TOOL_PATH=/opt/airflow/sync_tool
    - DBT_PROJECT_PATH=/opt/airflow/dbt_projects/order_tracking
    - DBT_VENV_PATH=/home/airflow/.local
    
    # Source Database (MySQL) - Connection Strategy
    - DB_HOST=us-west-2.ro.db.uniuni.com.internal
    - DB_USER=jasleentung
    - DB_PASSWORD=86a05b5772c55c83c9db74a01c01441f
    - DB_DATABASE=kuaisong
    
    # SSH Bastion Configuration
    - SSH_BASTION_HOST=35.83.114.196
    - SSH_BASTION_USER=jasleentung
    - SSH_BASTION_KEY_PATH=/keys/jasleentung.pem
    
    # Redshift Credentials & DBT
    - REDSHIFT_USER=sett_ddl_owner
    - "REDSHIFT_PASSWORD=Qx2[,y4*voli3)M>"
    - REDSHIFT_QA_USER=sett_ddl_owner
    - "REDSHIFT_QA_PASSWORD=Qx2[,y4*voli3)M>"
    - DBT_REDSHIFT_DB=dw
    - DBT_REDSHIFT_HOST=redshift-dw.uniuni.com
    - DBT_REDSHIFT_PORT=5439
    
    # DBT Tunnel (Disabled for Server deployment - Direct Redshift Access)
    - DBT_USE_SSH_TUNNEL=false
    # Note: REDSHIFT_SSH_KEY_PATH is still defined for backward compatibility
    - REDSHIFT_SSH_KEY_PATH=/keys/jasleentung.pem
    
    # Alert SMTP â€” extraction lag emails
    - ALERT_SMTP_HOST=smtp.office365.com
    - ALERT_SMTP_PORT=587
    - ALERT_SMTP_USER=jasleen.tung@uniuni.com
    - ALERT_SMTP_PASSWORD=Guruji@2312ninety
    - ALERT_EMAIL_TO=jasleen.tung@uniuni.com

    # S3 Configuration for Backfill
    - S3_BUCKET_NAME=redshift-dw-qa-uniuni-com
    - S3_REGION=us-west-2
    # Try multiple variants for credentials
    - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID_QA:-${AWS_ACCESS_KEY_ID}}
    - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY_QA:-${AWS_SECRET_ACCESS_KEY}}
    # Also set S3_ prefixed vars for AppConfig compatibility
    - S3_ACCESS_KEY=${AWS_ACCESS_KEY_ID_QA:-${AWS_ACCESS_KEY_ID}}
    - S3_SECRET_KEY=${AWS_SECRET_ACCESS_KEY_QA:-${AWS_SECRET_ACCESS_KEY}}

  volumes:
    - ./dags:/opt/airflow/dags
    - ./dbt_projects:/opt/airflow/dbt_projects
    - ./logs:/opt/airflow/logs
    # CRITICAL: Mount code root to SYNC_TOOL_PATH so 'cd {SYNC_TOOL_PATH}' works
    - ../:/opt/airflow/sync_tool
    # Mount SSH Key (Using absolute path /home/ubuntu instead of ~ or Windows paths)
    - /home/ubuntu/etl/etl_dw/ssh/jasleentung.pem:/keys/jasleentung.pem:ro

  depends_on:
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create --username admin --password admin123 --firstname Admin --lastname User --role Admin --email admin@example.com
    depends_on:
      postgres:
        condition: service_healthy

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      # Fixed syntax error (added missing quote)
      - "8081:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler

volumes:
  postgres-db-volume: